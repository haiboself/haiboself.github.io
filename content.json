{"meta":{"title":"Hexo","subtitle":"","description":"","author":"John Doe","url":"http://example.com","root":"/"},"pages":[{"title":"","date":"2023-03-26T10:29:04.140Z","updated":"2023-03-26T10:29:04.118Z","comments":true,"path":"baidu_verify_codeva-Tqg6gNHTbd.html","permalink":"http://example.com/baidu_verify_codeva-Tqg6gNHTbd.html","excerpt":"","text":"8829967d7f001f33faa792a1ca08675f"},{"title":"","date":"2023-03-26T10:43:36.072Z","updated":"2023-03-26T10:43:36.050Z","comments":true,"path":"googlea9a78cff5ddec016.html","permalink":"http://example.com/googlea9a78cff5ddec016.html","excerpt":"","text":"google-site-verification: googlea9a78cff5ddec016.html"},{"title":"书单","date":"2022-11-24T14:15:06.162Z","updated":"2022-11-24T14:15:06.161Z","comments":false,"path":"index.html","permalink":"http://example.com/index.html","excerpt":"","text":""},{"title":"","date":"2022-11-25T02:10:00.996Z","updated":"2022-11-25T02:10:00.996Z","comments":true,"path":"links.json","permalink":"http://example.com/links.json","excerpt":"","text":"{\"创造狮\":{\"link\":\"http://chuangzaoshi.com/\",\"avatar\":\"/images/favatar/chuangzaoshi-logo.png\",\"desc\":\"为创意工作者而设计\"},\"腾讯设计导航\":{\"link\":\"http://idesign.qq.com/\",\"avatar\":\"/images/favatar/idesign-logo.png\",\"desc\":\"网罗全网高逼格的设计站点\"}}"},{"title":"关于","date":"2022-11-25T14:05:53.455Z","updated":"2022-11-25T14:05:53.432Z","comments":false,"path":"about/index.html","permalink":"http://example.com/about/index.html","excerpt":"","text":"我是谁 陕西宝鸡人 2013.09-2017.07 西北大学-软件工程专业 2017.07-2020.03 杭州挖财网络科技有限公司-数据平台-数据研发工程师 2020.03-2022.09 北京三快在线(美团)科技公司-数据平台-数据系统研发工程师 2020.09-至今 北京罗克维尔斯科技(理想汽车)-数据平台-大数据开发工程师 有哪些工作技能毕业后一直从事于大数据领域的开发工作，参与过数据平台 0-1 的建设，也参与过成熟平台进一步的演进。主要负责过以下工作内容： 数据集成、数据交换 离线工作流调度 olap、spark 埋点系统 数据质量、数据治理、大数据测试 如何找到我&#104;&#97;&#x69;&#98;&#x6f;&#x2d;&#x73;&#x65;&#108;&#102;&#64;&#x31;&#x36;&#51;&#46;&#99;&#x6f;&#x6d; &#x2F;** * ┏┓ ┏┓+ + * ┏┛┻━━━┛┻┓ + + * ┃ ┃ * ┃ ━ ┃ ++ + + + * ████━████ ┃+ * ┃ ┃ + * ┃ ┻ ┃ * ┃ ┃ + + * ┗━┓ ┏━┛ * ┃ ┃ * ┃ ┃ + + + + * ┃ ┃ Code is far away from bug with the animal protecting * ┃ ┃ + 神兽保佑,永无bug * ┃ ┃ * ┃ ┃ + * ┃ ┗━━━┓ + + * ┃ ┣┓ * ┃ ┏┛ * ┗┓┓┏━┳┓┏┛ + + + + * ┃┫┫ ┃┫┫ * ┗┻┛ ┗┻┛+ + + + *&#x2F;"},{"title":"分类","date":"2022-11-25T00:36:45.865Z","updated":"2022-11-25T00:36:45.865Z","comments":false,"path":"categories/index.html","permalink":"http://example.com/categories/index.html","excerpt":"","text":""},{"title":"书单","date":"2022-11-24T14:22:20.370Z","updated":"2022-11-24T14:22:20.370Z","comments":false,"path":"books/index.html","permalink":"http://example.com/books/index.html","excerpt":"","text":""},{"title":"友情链接","date":"2022-11-25T02:07:09.723Z","updated":"2022-11-25T02:07:09.723Z","comments":true,"path":"links/index.html","permalink":"http://example.com/links/index.html","excerpt":"","text":""},{"title":"Repositories","date":"2022-11-25T02:03:16.310Z","updated":"2022-11-25T02:03:16.310Z","comments":false,"path":"repository/index.html","permalink":"http://example.com/repository/index.html","excerpt":"","text":""},{"title":"标签","date":"2022-11-25T00:36:51.993Z","updated":"2022-11-25T00:36:51.993Z","comments":false,"path":"tags/index.html","permalink":"http://example.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"StarRocks COMPACTION 机制解读 1","slug":"yuque/StarRocks COMPACTION 机制解读1","date":"2023-03-25T13:18:08.000Z","updated":"2023-03-25T13:40:00.598Z","comments":true,"path":"2023/03/25/yuque/StarRocks COMPACTION 机制解读1/","link":"","permalink":"http://example.com/2023/03/25/yuque/StarRocks%20COMPACTION%20%E6%9C%BA%E5%88%B6%E8%A7%A3%E8%AF%BB1/","excerpt":"","text":"引言StarRocks，它的计算部分借鉴了 impala[1]，存储部分借鉴了 Google Mesa[2]。而 Compaction 机制正是来源于 Google Mesa。Mesa 是一个分析型的数据仓库系统（highly scalable analytic data warehousing system），主要用于谷歌的广告业务（分析、报表）中。它非常强调“实时性”，它要支持： Near Real-Time Update Throughput ：实时的大批量的数据更新 实时更新：must support continuous updates qps 达到几百万：millions of rows updated per second Query Performance：point query 的 p99 要达到几百毫秒，就是要满足这种低时延的场景 Atomic Update … 所以它采用了类似 LSM-tree[3] 的这种数据结构，牺牲了一些查询性能，但是换取了写性能的提升。而这种结构就避免不了要进行数据的 “Compaction”。所以 StarRocks 会在后台通过 Compaction 机制不断将小文件合并成有序的大文件，同时也会处理数据的删除、更新等操作。 备注：下图是 Lsm-tree 的 compaction，思想类似，但是 StarRocks 和 Mesa 的机制有不同之处。(LSM-tree compaction) 存储结构介绍StarRocks 存储结构由 4 层结构组成。 Rowset Rowset 是 Tablet 中一次数据变更的数据集合[4]，数据变更包括了数据导入、删除、更新等。所谓一次数据变更就是一次导入事务，比如一次 Stream Load 导入就对应一个新的 Rowset。 Rowset 按版本信息进行记录。版本由 start_version、end_version 两个属性构成，维护数据变更的记录信息。通常用来表示 Rowset 的版本范围。 在一次新导入后生成一个 start_version，end_version 相等的 Rowset 在 Compaction 后生成一个带范围的 Rowset 版本 Segment 表示 Rowset 中的数据分段，多个 Segment 构成一个 Rowset。Segment 文件可以有多个，一般按照大小进行分割，默认为 256MB。 Segment File(类似 SSTable) 是**真正的数据文件[5]**，是一个“类 parquet” 的列存储格式。整体的文件格式分为数据区域，索引区域和 footer 三个部分，如下图所示： 数据写入流程 当一个 Memtable 写满时(默认为 100M)，调用 RowsetWriter 将 Memtable 的数据会 flush 到磁盘上 RowsetWriter 调用 SegmentWriter, 将数据和索引写入到磁盘 Compaction To enforce update atomicity, Mesa uses a multi-versioned approach. Mesa applies updates in order by version number, ensuring atomicity by always incorporating an update entirely before moving on to the next update. Users can never see any effects from a partially incorporated update.[2] 如上 Mesa 中所述，为了实现开头说的数据更新的“原子性”(Atomic Update)，数据采用了 MVCC (想想 rowset 中的版本，一次导入是一个事物，完成后形成一个版本)，实现了 snapshot isolation。但是这也带来了一些问题： 过多的数据版本会带来更多的存储开销：一条数据如果多次更新，被存储在不同的 rowset 中，便会发生冗余的存储。 过多的数据版本会带来查询时效的降低：查询采用 Merge on read 的模式，在查询时要 merge 所有版本的数据来产生最终的聚合结果。如果采用预聚合的方式（copy on write），每次更新都进行预聚合，那么成本也非常高昂。 所以为了解决以上的问题来平衡读写，Mesa 采用了 2 级 Compaction 机制[2]，这样可以： 合并多个数据版本，避免在读取时大量的 Merge 操作。这可以看作讲查询时进行所有 merge 的时间平摊到了一个更长的时间轴上，从而提高了查询时效。 避免大量的数据版本导致的随机 I 当然 StarRocks 借鉴了这种机制但并不完全一样：会根据一定的策略对这些 Rowset 进行合并，将小文件合并成大文件，进而提升查询性能。compaction 也是分两种： Cumulative Compaction： cumulative compaction 主要负责将多个最新导入的 rowset 合并成较大的 rowset Base Compaction：将 cumulative compaction 产生的 rowset 合入到 start version 为 0 的基线数据版本（Base Rowset）中，是一种开销较大的 compaction 操作 这两种 compaction 的边界通过 cumulative point 来确定。base compaction 会将 cumulative point 之前的所有 rowset 进行合并，cumulative compaction 会在 cumulative point 之后选择相邻的数个 rowset 进行合并[6] 合并流程合并流程大体分为 4 步，其中 Base Compaction 和 Cumulative Compaction 主要是第二步计算 Cumulative_point[6] 不一样。 Compaction 任务的产生采用了生产者-消费者模式。 Compaction 任务是一个** IO 密集型**任务，为了保证其不占用过多 io 资源[8]，限制了每个磁盘上能够同时进行的 Compaction 任务数量，默认 base_compaction_num_threads_per_disk &#x3D; 1 cumulative_compaction_num_threads_per_disk &#x3D; 1 Compaction 任务同时也是**内存密集型[8]**任务（本质是多个有序文件的多路归并排序过程），为了限制内存使用 内存使用量与本次 compaction 任务合并的文件数量有关，因此限制了每次任务包含的数据版本数，默认（singleton 其实是 mesa 中的概念[13]，对应 rowset） max_base_compaction_num_singleton_deltas &#x3D; 100。 max_cumulative_compaction_num_singleton_deltas &#x3D; 1000 为了调节 BE 节点 compaction 的内存使用量增加了对 compaction 任务提交的 permission 机制。 在一轮任务生产过程中，会从每个磁盘各选择出一个 tablet 执行 compaction 任务，目前的策略是每生成 9 个 CC 任务，生成一个 BC 任务。如何选择 Tablet 参见 [6] 这种方式其实不适合数据高频更新、高频点查的场景，比如主键模型的场景。为什么呢，因为高频更新就会生成大量的 rowset，一方面会增加 compaction 的压力；另一方面查询时就要 merge 更多的版本，查询时效受影响。所以因此 StarRocks 针对这个场景（主键模型）做了优化，见[11]。 Cumulative Compactioncumulative compaction 不会将 delete 操作删除的数据行进行真正地删除，这部分工作会在 base compaction 中进行。Doris 的 cumulative compaction 每次会在 cumulative point 之后选择相邻的数个 rowset 进行合并，主要包含 5 个步骤 Base CompactionDoris 的 base compaction 会将 cumulative point 之前的所有 rowset 进行合并，base compaction 过程中会将 delete 操作删除的数据行真正地删除。 Compaction 问题排查如图[12]：这 3 种导入方式都会频繁生成大量的版本即 rowset 文件，增加 compaction 的压力 insert into values 和 jdbc 频繁导入，每次导入都是一个事务，会形成一个 rowset。试想通过 insert into 每次导入 1 条数据，那么 100w 数据就会生成 100w 个 rowset，需要 compaction 去合并。 同理，高频 stream load 可能导致 compaction 的速度跟不上导入的速度，形成导入的压力，影响查询性能，因为查询时候就得做更多版本 merge。 Compaction 出现压力时，调优可参考：[7][8][9] 引用 Impala Mesa: Geo-Replicated, Near Real-Time, Scalable Data Warehousing DB 存储引擎知识系列之三：LSM-Tree 存储引擎详细分解_mob604756f33d49 的技术博客_51CTO 博客 【Doris 全面解析】存储层设计介绍 2——写入流程、删除流程分析 【Doris 全面解析】存储层设计介绍 1——存储结构设计解析 技术解析 | Doris Compaction 机制解析百度百度开发者中心_InfoQ 写作社区 Doris 最佳实践-Compaction 调优(1) - 墨天轮 Doris 最佳实践-Compaction 调优(2) - 墨天轮 Doris 最佳实践-Compaction 调优(3) - 墨天轮 编程小梦|Google Mesa 论文解读 StarRocks 技术内幕：实时更新与极速查询如何兼得 2&#x2F;22 19:00 直播 | StarRocks 实战系列 Ep.2—导入优化＆问题排查（转发、打卡还可以获得积分奖品！） https://blog.csdn.net/qq_35200943&#x2F;article&#x2F;details&#x2F;127498751 https://www.slidestalk.com/doris.apache/61900?video","categories":[{"name":"olap","slug":"olap","permalink":"http://example.com/categories/olap/"}],"tags":[{"name":"starrocks","slug":"starrocks","permalink":"http://example.com/tags/starrocks/"}]},{"title":"StarRocks 2.2.9 Hive外表元数据缓存刷新原理","slug":"2.2.9版本Hive外表元数据缓存刷新原理","date":"2022-12-04T12:34:27.000Z","updated":"2023-03-25T13:32:24.869Z","comments":true,"path":"2022/12/04/2.2.9版本Hive外表元数据缓存刷新原理/","link":"","permalink":"http://example.com/2022/12/04/2.2.9%E7%89%88%E6%9C%ACHive%E5%A4%96%E8%A1%A8%E5%85%83%E6%95%B0%E6%8D%AE%E7%BC%93%E5%AD%98%E5%88%B7%E6%96%B0%E5%8E%9F%E7%90%86/","excerpt":"","text":"1、为什么要做外表刷新StarRocks(简称 SR) 具备通过创建外表的方式来查询 hive 数据。SR fe 在生成查询计划时需要从 hive metastore 读取对应 hive 表、分区的元数据，为了获得更快的查询时效，SR 将已获取的 hive 元数据缓存在了 Fe 的内存中（使用 Guava LoadingCache 实现），从而减少对 hive metastore 的请求次数。这样就产生一个问题，hive 表元数据变更时导致和 SR 缓存的元数据不一致，从而导致 SR 查询异常。为了保持缓存元数据和 hive metastore 一致，SR 提供了两种刷新机制 自动刷新，也有 2 种方式 设置“缓存过期时间”或者“缓存自动刷新间隔时间” 自动增量更新元数据缓存，主要是通过定期消费Hive Metastore的 event 来实现，需要 hive 版本 &gt; 3.1.2（待验证） 主动刷新，主动执行刷新 sql 来强制更新 SR 缓存的某张表的元数据，有2个操作 refresh external table xxx 来更新级别的表的信息 refresh external table xxx partion(yyy) 来更新缓存的分区的信息 因为所有 hive 表不是同一个时间变更的，所以 1.a 无法适用，其次 hive 版本也不满足 1.b 的要求。所以我们选择了“手动刷新”的方式，在一张 hive 表变更后通过系统手动刷新该表在 SR 的元数据。 2、SR外表元数据的缓存设计SR 对于 hive 元数据缓存在每个 Fe 内存中，适用 Guava LoadingCache 实现，共有 5 种(5个 loadingCache)缓存，可以分为两类 表级别缓存，缓存 size &lt;&#x3D;1k，即最多缓存 1k 张表的信息，超过按 LRU 策略置换；有 3 种 partitionKeysCache：一个表中所有的分区信息 tableStatsCache：表数据的信息，如 numRows、totalFileBytes tableColumnStatsCache：字段信息，如字段类型、字段的最大最小值等 分区级别缓存，缓存** size &lt;&#x3D;100w**，即最多缓存 100w 个分区的信息，超过按 LRU 策略置换；有 2 种 partitionsCache：一个分区对应的底层文件信息，如 hdfs 目录 partitionStatsCache：一个分区的说句信息，如 numRows、totalFileBytes1234567891011121314// HivePartitionKeysKey =&gt; ImmutableMap&lt;PartitionKey -&gt; PartitionId&gt;// for unPartitioned table, partition map is: ImmutableMap&lt;&gt;.of(new PartitionKey(), PartitionId)LoadingCache&lt;HivePartitionKeysKey, ImmutableMap&lt;PartitionKey, Long&gt;&gt; partitionKeysCache;// HivePartitionKey =&gt; PartitionsLoadingCache&lt;HivePartitionKey, HivePartition&gt; partitionsCache;// statistic cache// HiveTableKey =&gt; HiveTableStatisticLoadingCache&lt;HiveTableKey, HiveTableStats&gt; tableStatsCache;// HivePartitionKey =&gt; PartitionStatisticLoadingCache&lt;HivePartitionKey, HivePartitionStats&gt; partitionStatsCache;// HiveTableColumnsKey =&gt; ImmutableMap&lt;ColumnName -&gt; HiveColumnStats&gt;LoadingCache&lt;HiveTableColumnsKey, ImmutableMap&lt;String, HiveColumnStats&gt;&gt; tableColumnStatsCache; 2 类缓分别有其对应的刷新动作，连接某台 fe 执行刷新动作时，SR 会在所有 fe 上执行缓存刷新操作 sql: refresh external table xxx 刷新表级缓存信息 sql: refresh external table xxx partition(yyy) 刷新分区级缓存信息 3、缓存相关流程和存在的问题3.1、缓存使用的流程如 之中所述，使用hive 外表时需要创建 hive resource，它和缓存的关系如图: ResourceMgr 管理所有 resource HiveRepository 中管理每个 resource 对应的 HiveMetaCache HiveMetaCache 依赖 Resource 而存在，如果 Resource 被删，HiveRepository 会清除对应的 HiveMetaCache 在 SR 查询 sql 生成执行计划时，会通过 catalog 调用 HiveRepository 查询对应 resource 的元数据缓存，如果缓存信息缺乏，会即时的去 hiveMetaStore 获取并更新缓存; 3.2、主动刷新动作的执行流程刷新动作执行的整体流程： 刷新本 fe 缓存 成功后请求其他 fe 刷新缓存 其他 fe 刷新失败时也不会使本 fe 缓存回滚，所以这里可能存在 fe 之间缓存不一致的问题1234567891011121314151617181920212223242526272829303132333435public void refreshExternalTable(RefreshExternalTableStmt stmt) throws DdlException &#123; // 刷新本 fe 缓存 refreshExternalTable(stmt.getDbName(), stmt.getTableName(), stmt.getPartitions()); List&lt;Frontend&gt; allFrontends = Catalog.getCurrentCatalog().getFrontends(null); Map&lt;String, Future&lt;TStatus&gt;&gt; resultMap = Maps.newHashMapWithExpectedSize(allFrontends.size() - 1); for (Frontend fe : allFrontends) &#123; if (fe.getHost().equals(Catalog.getCurrentCatalog().getSelfNode().first)) &#123; continue; &#125; resultMap.put(fe.getHost(), refreshOtherFesTable(new TNetworkAddress(fe.getHost(), fe.getRpcPort()), stmt.getDbName(), stmt.getTableName(), stmt.getPartitions())); &#125; // 成功后请求其他 fe 刷新缓存 String errMsg = &quot;&quot;; for (Map.Entry&lt;String, Future&lt;TStatus&gt;&gt; entry : resultMap.entrySet()) &#123; try &#123; TStatus status = entry.getValue().get(); if (status.getStatus_code() != TStatusCode.OK) &#123; String err = &quot;refresh fe &quot; + entry.getKey() + &quot; failed: &quot;; if (status.getError_msgs() != null &amp;&amp; status.getError_msgs().size() &gt; 0) &#123; err += String.join(&quot;,&quot;, status.getError_msgs()); &#125; errMsg += err + &quot;;&quot;; &#125; &#125; catch (Exception e) &#123; errMsg += &quot;refresh fe &quot; + entry.getKey() + &quot; failed: &quot; + e.getMessage(); &#125; &#125; if (!errMsg.equals(&quot;&quot;)) &#123; ErrorReport.reportDdlException(ErrorCode.ERROR_REFRESH_EXTERNAL_TABLE_FAILED, errMsg); &#125;&#125; refresh external table xxx 刷新表级缓存信息流程:refresh external table xxx partition(yyy) 刷新分区级缓存信息流程：同上，区别在于刷新失败不清除缓存。 3.3、我们的使用流程 缓存设为不失效和不自动更新 调度任务执行成功后触发刷新动作 先refresh external table xxx 刷新表级缓存信息 成功后执行 refresh external table xxx partition(yyy) 刷新本任务新生成的分区信息 3.4、现有流程引发的问题1）分区过多（如几十万）的表刷新访问 metastore 太慢，如 3.2 刷新表级缓存流程所述，会一直占有写锁，导致其他表的刷新动作一直等待获取锁直至 mysql client 连接超时或者获取不到新连接，缓存得不到刷新，查询不到最新的数据。 123456789101112131415161718192021222324252627282930public void refreshTable(HiveMetaStoreTableInfo hmsTable) throws DdlException &#123; String dbName = hmsTable.getDb(); String tableName = hmsTable.getTable(); Table.TableType tableType = hmsTable.getTableType(); List&lt;Column&gt; partColumns = getPartitionColumns(hmsTable); List&lt;String&gt; columnNames = getAllColumnNames(hmsTable); HivePartitionKeysKey hivePartitionKeysKey = new HivePartitionKeysKey(dbName, tableName, tableType, partColumns); HiveTableKey hiveTableKey = HiveTableKey.gen(dbName, tableName); HiveTableColumnsKey hiveTableColumnsKey = new HiveTableColumnsKey(dbName, tableName, partColumns, columnNames, tableType); Catalog.getCurrentCatalog().getMetastoreEventsProcessor().getEventProcessorLock().writeLock().lock(); try &#123; ImmutableMap&lt;PartitionKey, Long&gt; partitionKeys = loadPartitionKeys(hivePartitionKeysKey); partitionKeysCache.put(hivePartitionKeysKey, partitionKeys); tableStatsCache.put(hiveTableKey, loadTableStats(hiveTableKey)); tableColumnStatsCache.put(hiveTableColumnsKey, loadTableColumnStats(hiveTableColumnsKey)); // for unpartition table, refresh the partition info, because there is only one partition if (partColumns.size() &lt;= 0) &#123; HivePartitionKey hivePartitionKey = new HivePartitionKey(dbName, tableName, tableType, new ArrayList&lt;&gt;()); partitionsCache.put(hivePartitionKey, loadPartition(hivePartitionKey)); partitionStatsCache.put(hivePartitionKey, loadPartitionStats(hivePartitionKey)); &#125; &#125; catch (Exception e) &#123; LOG.warn(&quot;refresh table cache failed&quot;, e); throw new DdlException(&quot;refresh table cache failed: &quot; + e.getMessage()); &#125; finally &#123; Catalog.getCurrentCatalog().getMetastoreEventsProcessor().getEventProcessorLock().writeLock().unlock(); &#125;&#125; 2）补数任务执行刷新分区动作失败后，导致表级缓存和分区级缓存不一致，查询失败表级刷新动作，会刷新 partitionKeysCache，所以它执行成功就有表中有哪些分区的信息，这时： 当是新增&#x2F;删除分区时，即使刷新分区动作失败，fe 使用时也会自动加载对应分区信息所以不影响使用 当是更新已有分区时，已有分区信息若在分区信息partitionsCache缓存中，那刷新分区动作失败，fe 使用时候拿到的是旧缓存所以查询会异常。 这就是现在每天调度任务刷新多级分区失败但是不影响使用，补数时刷新多级分区失败导致查询异常的原因。 3）刷新表级缓存动作失败，但是不影响使用原因就是如 3.2 中所述，表级刷新失败会导致清除该表的所有缓存，这样 fe 使用时会按需重新从 hive metastore 中加载，所以不影响使用 4、终级方案无论用哪种方式，都是为了使 hive meta store 变更时，变更能及时同步到 sr 的缓存中，现有流程也是这么做的，只是做到到调度层。最好的方式可以下沉到 hive 层，对调度和用户透明。有 2 种方式可以实现，待探索 消费 hive metastore 的 binlog，触发对 sr 的刷新动作 使用 hive meta hook 机制来触发对 sr 的刷新动作","categories":[{"name":"olap","slug":"olap","permalink":"http://example.com/categories/olap/"}],"tags":[{"name":"starrocks","slug":"starrocks","permalink":"http://example.com/tags/starrocks/"}]},{"title":"深入剖析kubernetes 1-4章阅读总结","slug":"深入剖析kubernetes1-4章读书笔记","date":"2022-10-30T06:41:51.000Z","updated":"2023-03-25T13:32:24.870Z","comments":true,"path":"2022/10/30/深入剖析kubernetes1-4章读书笔记/","link":"","permalink":"http://example.com/2022/10/30/%E6%B7%B1%E5%85%A5%E5%89%96%E6%9E%90kubernetes1-4%E7%AB%A0%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/","excerpt":"","text":"容器的来龙去脉PaaSPaas 解决的是应用程序的托管、打包与分发问题，强调零停机时间部署、自动规模伸缩与负载均衡等功能。用户可以在云服务商租一批虚拟机，然后可以用脚本或手动的方式在上面部署应用。以 Cloud Foundry 这个 PaaS 项目为例，它的核心组件就是一套应用的打包和分发机制。 用户把应用、启动脚本等打包到一个压缩包中，上传后 Cloud Foundry 会通过调度器选择一个合适的虚拟机，然后通知这个机器的 Agent 下载应用压缩包并启动 由于需要在一个虚拟机上启动多个应用，Cloud Foundry 会调用操作系统的 Cgroups 和 Namespace 机制为每个应用单独创建一个隔离环境唤做“沙盒”，然后在里面启动应用进程。 这样就把多个应用在虚拟机中互不干扰的自动运行了起来，**这些“隔离环境”，就是“容器”，**后文具体介绍。 DockerDocker 容器和 Cloud Foundry “沙箱”没有本质区别，它主要解决了 PaaS 中“应用打包难”的问题。Docker 之前应用打包难： 用户必须为每种语言、每种框架，甚至每个版本为一个一个打好的包。 打包过程没有章法可循，且在本地良好运行的应用，需要做很多配置和修改才能在 Paas 中运行。 Docker 的解决方案: 镜像，就是打包了应用程序 + 需要的整个操作系统，从而保证了本地环境和云端环境的高度一致，避免了用户通过“试错”来匹配不同运行环境之间差异的过程。 Docker 只是解决了打包部分的问题，但是并不负责“分发”、“应用部署”。因此，Docker 提供了 Swarm 工具来做集群管理，swarm 集群由管理节点（manager）和工作节点（work node）构成。 swarm mananger：负责整个集群的管理工作包括集群配置、服务管理等所有跟集群有关的工作。 work node：即图中的 available node，主要负责运行相应的服务来执行任务（task）。 Compose（Fig）项目来做容器编排。 容器编排：指用户通过某些工具或配置来完成一组虚拟机以及关联资源的定义、配置、创建、删除等工作。 对 docker 而言，编排就是对 docker 容器的一系列定义、配置和创建动作的管理。比如定义多个容器之间的依赖关系[2] K8s“容器的价值非常有限，真正有价值的是容器编排”，很多项目（Yarn，Mesos，Docker Swarm）擅长的是把一个容器按照某种规则放置到某个最佳节点上执行，这种功能成为“调度”。K8s 擅长的是编排，即按照用户的意愿和整个系统的规则，完全自动化地处理好容器之间的各种关系。[14] 容器技术基础容器技术的核心功能，就是通过约束和修改进程的“动态表现”，为其创造一个“边界”。以 Linux 容器为例 NameSpace 技术是用来进行隔离资源的主要方法。 Cgroups 技术是用来限制（限制 cpu、内存、存储等资源）的主要手段 Namespace[3]Linux Namespace 提供了一种内核级别隔离系统资源的方法，通过将系统的全局资源放在不同的 Namespace 中，来实现资源隔离的目的。 目前提供了 6 种系统资源的隔离机制 Mount：隔离文件系统 UTS：隔离主机名和域名信息 IOC：隔离进程间通信 PID：隔离进程的 ID Network：隔离网络资源 User：隔离用户和用户组的 ID 以 PID 隔离[7]为例，举个例子：1）在宿主机启动一个 docker 容器 1docker run -it busybox /bin/sh 2）查看进程号左边是容器(隔离出的空间)的中的进程号为 1，对应到右边宿主机中真实的进程好为 307893）实际上，docker 利用 Namespace 机制，施了一个障眼法，对被隔离的进程空间动了手脚，使得这些进程看到的是“重新计算过的 PID” 实际上，在创建进程时候传递对应的 NameSpace 参数，来对进程上下文施展各种障眼法，这样进程就只能看到所属 NameSpace 下的资源信息。容器就使用了这种技术，Docker 启动的还是原来的应用进程，只不过在创建这些进程时，Docker 为他们加上了各种各样的 Namespace 参数。所以：容器其实只是一种特殊的进程，用户应用实际上就是容器中 PID&#x3D;1 的进程。 CgroupsLinux Cggroups(Linux control groups) 最主要的作用就是限制一个进程组能够使用的资源上限，包括 CPU、内存、磁盘、网络带宽等等。cgroup 通过以下组件来抽象进程和资源[9] task：任务，对应进程 subsystem：子系统，具体的资源控制器，控制某个特定的资源使用，如 cpu 子系统控制 cpu 使用时间 cgroup：控制组，一组任务和子系统的关联关系，标识对这些任务进行怎样的资源控制策略 hierarchy：层级树，一些列 cgroup 组成的树形结构。 进程和 cgroup 是多对多的关系[10]： Cgroup 使用示例：1）Linux 下 cgroups 以文件和目录方式组织在 &#x2F;sys&#x2F;fs&#x2F;cgroup 下： 12345678910111213[root@VM-16-5-centos ~]# mount -t cgroupcgroup on /sys/fs/cgroup/systemd type cgroup (rw,nosuid,nodev,noexec,relatime,xattr,release_agent=/usr/lib/systemd/systemd-cgroups-agent,name=systemd)cgroup on /sys/fs/cgroup/perf_event type cgroup (rw,nosuid,nodev,noexec,relatime,perf_event)cgroup on /sys/fs/cgroup/blkio type cgroup (rw,nosuid,nodev,noexec,relatime,blkio)cgroup on /sys/fs/cgroup/net_cls,net_prio type cgroup (rw,nosuid,nodev,noexec,relatime,net_cls,net_prio)cgroup on /sys/fs/cgroup/devices type cgroup (rw,nosuid,nodev,noexec,relatime,devices)cgroup on /sys/fs/cgroup/cpuset type cgroup (rw,nosuid,nodev,noexec,relatime,cpuset)cgroup on /sys/fs/cgroup/memory type cgroup (rw,nosuid,nodev,noexec,relatime,memory)cgroup on /sys/fs/cgroup/freezer type cgroup (rw,nosuid,nodev,noexec,relatime,freezer)cgroup on /sys/fs/cgroup/cpu,cpuacct type cgroup (rw,nosuid,nodev,noexec,relatime,cpu,cpuacct)cgroup on /sys/fs/cgroup/pids type cgroup (rw,nosuid,nodev,noexec,relatime,pids)cgroup on /sys/fs/cgroup/hugetlb type cgroup (rw,nosuid,nodev,noexec,relatime,hugetlb)cgroup on /sys/fs/cgroup/rdma type cgroup (rw,nosuid,nodev,noexec,relatime,rdma) 2）以 cpu 使用限制为例，在 cpu 下创建 container 目录（控制组） 123[root@VM-16-5-centos container]# lscgroup.clone_children cpuacct.stat cpuacct.usage_all cpuacct.usage_percpu_sys cpuacct.usage_sys cpu.cfs_period_us cpu.shares notify_on_releasecgroup.procs cpuacct.usage cpuacct.usage_percpu cpuacct.usage_percpu_user cpuacct.usage_user cpu.cfs_quota_us cpu.stat tasks 3）限制 cpu 的使用，表示在 100ms 的时间内，被该控制组限制的进程只能使用 20ms 的 cpu 时间，可以将需要限制的进程 id 写入到 tasks 文件中 123echo 100000 &gt; /sys/fs/cgroup/cpu/container/cpu.cfs_period_usecho 20000 &gt; /sys/fs/cgroup/cpu/container/cpu.cfs_quota_usecho $(pid) &gt; /sys/fs/cgroup/cpu/container/tasks Docker 创建 linux 容器时，只需要在每个子系统下创建对应的控制组（新建目录），然后在启动容器进程后，把整个进程的 PID 天蝎到对应控制组的 tasks 文件中就可以完成资源的限制。如： 1234567891011[root@VM-16-5-centos container]docker run -it --cpu-period=100000 --cpu-quota=20000 busybox /bin/sh[root@VM-16-5-centos docker-4cd72e800ed85e9502818dfc1bec238f653da27e10ce61f74d50fb501f819200.scope]# lscgroup.clone_children cpuacct.usage_all cpuacct.usage_sys cpu.sharescgroup.procs cpuacct.usage_percpu cpuacct.usage_user cpu.statcpuacct.stat cpuacct.usage_percpu_sys cpu.cfs_period_us notify_on_releasecpuacct.usage cpuacct.usage_percpu_user cpu.cfs_quota_us tasks[root@VM-16-5-centos docker-4cd72e800ed85e9502818dfc1bec238f653da27e10ce61f74d50fb501f819200.scope]# cat cpu.cfs_quota_us20000[root@VM-16-5-centos docker-4cd72e800ed85e9502818dfc1bec238f653da27e10ce61f74d50fb501f819200.scope]# cat cpu.cfs_period_us100000[root@VM-16-5-centos docker-4cd72e800ed85e9502818dfc1bec238f653da27e10ce61f74d50fb501f819200.scope]# Rootfs[13]容器进程应该“看到”，一个完全隔离、独立的文件环境，而不是继承自宿主机的文件系统。在 Linux 中要实现这一点，要借助 chroot(change root file system) 命令，可以将指定目录挂载为指定容器进程的根目录。一般会在容器的根目录下“挂在一个完整的操作系统的文件系统目录”，这个用来为容器进程提供隔离后执行环境的文件系统，就是“容器镜像”，即 rootrs（根文件系统）； 这些文件不包含操作系统内核，所以区别于虚拟机同一台机器上所有容器都是共享操作系统内核的。 （容器和虚拟机的对比） 打包操作系统文件目录，赋予了容器的一致性：无论在本地、云端或其他机器上，只需解压打包好的镜像，这个应用运行所需要的完整的执行环境就能重现。 总结容器实际是由 Linux Namespace，Linux Cgroup 和 Rootfs 3 种技术构建出来的进程的隔离环境，一个运行的 Linux 容器，可以被一分为二的看待： 容器镜像：一组联合挂载在 &#x2F;var&#x2F;lib&#x2F;docker&#x2F;aufs&#x2F;mnt 上的 rootfs，是容器的静态视图 容器运行时：一个由 Namespace + Cgroups 构成的隔离环境，是容器的动态视图 K8s 的设计核心能力与定位在大规模集群中的各种任务之间运行，实际存在各种各样的关系。这些关系的处理才是作业编排和管理系统最困难的地方。K8s 的核心能力就是要解决这个问题，而不是拉取和部署镜像。K8s 的主要设计思路是： 以统一的方式抽象底层基础设施能力（计算、存储、网络等）；定义任务编排的各种关系（如亲密关系、访问关系、代理关系） 以声明式 api（yaml）的方式对外暴露，从而允许用户基于这些抽象构建自己的上层 平台（如自己的 paas） 所以，K8s 的本质是 “平台的平台”，一个帮助用户构建上层平台的基础平台。 如图，K8s 是如何定义任务编排的各种关系的 Pod 中的容器共享同一个 Network Namespace、同组 Volume，从而实现高效交换信息。 架构设计[11]分为 Master 控制节点和 Node 计算节点 Master：Controller Manager 负责容器编排；整个集群的数据，由 Api Server 处理后存储在 etcd 中 Node： kubelet 主要负责和容器运行时（如 Docker）交互，交互依赖 CRI(container runtime interface)远程调用接口。如 Docker 通过 OCI 容器运行时规范同 Linux 交互，把 CRI 请求翻译为对 Linux 系统的调用（操作 Namespace，Cgroups 等） Device Plugin：是 k8s 用来管理 GPU 等宿主机无力设备的主要组件，kubelet 通过 grpc 和其交互 CNI(container networking interface)：kubelet 调用其为容器配置网络 CSI(container storage interface)：kubelet 调用其为容器配置持久化存储 Pod[12]容器是一个单进程模型，容器不具备进程管理能力，所以一个容器中只包含一个应用进程是合理的；当几个应用（容器）之间具有协作关系，需要共享某些资源，必须放在一起去管理（如必须运行在同一台机器上）要怎么办，而 pod 就是 k8s 给出的解决方案。 Pod 实际上是个逻辑概念，无力上对应的是一组容器；除此之外，Pod 是 Kubernetes 分配资源的一个单位，因为里面的容器要共享某些资源，所以 Pod 也是 Kubernetes 的原子调度单位。 总结PaaS 解决应用：打包、分发部署、托管执行方面的问题 打包：缺乏标准，打包难问题 -&gt; docker 容器镜像（增量 rootfs、layer）解决了这个问题，成为事实上的打包标准； 执行：依赖 linux os 自身的 namespace、cgroups、chroot 实现的容器技术解决这方面的问题； namespace：隔离进程资源视图 cgroups：限制进程的资源使用 PaaS 的 3 个问题解决好后，K8s 解决更上层的问题：容器、应用编排。从而成为平台的平台；如 cloud foundry 和 k8s 的结合[15]： 引用 https://www.spiceworks.com/tech/cloud/articles/what-is-platform-as-a-service/ https:&#x2F;&#x2F;medium.com&#x2F;@krishnakummar&#x2F;creating-block-diagrams-from-your-docker-compose-yml-da9d5a2450b4 https://techtutorialsite.com/docker-vs-virtual-machines/ https://www.slideshare.net/AdamFitzGerald/cf-overview-gitpro https://www.slideshare.net/jaxLondonConference/run-your-java-apps-on-cloud-foundry-andy-piper-pivotal https://blogs.sap.com/2018/06/05/cloud-native-with-containers-and-kubernetes-part-2/ https://zhuanlan.zhihu.com/p/73248894 https://www.nginx.com/blog/what-are-namespaces-cgroups-how-do-they-work/ https://juejin.cn/post/6921299245685276686 https://tech.meituan.com/2015/03/31/cgroups.html https://www.cnblogs.com/luoahong/p/12330735.html http://dockone.io/article/9290 https://www.feiyiblog.com/2020/03/27/%E9%95%9C%E5%83%8F%E5%88%86%E5%B1%82%E7%BB%93%E6%9E%84%E7%90%86%E8%AE%BA%E8%AF%A6%E8%A7%A31/ https://blogs.sap.com/2018/10/19/cloud-foundry-and-kubernetes-where-do-they-differ-how-do-they-fit-together/ https://blogs.sap.com/2021/01/15/back-to-the-future-cloud-foundry-on-kubernetes/","categories":[{"name":"云原生","slug":"云原生","permalink":"http://example.com/categories/%E4%BA%91%E5%8E%9F%E7%94%9F/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"http://example.com/tags/k8s/"},{"name":"容器","slug":"容器","permalink":"http://example.com/tags/%E5%AE%B9%E5%99%A8/"},{"name":"docker","slug":"docker","permalink":"http://example.com/tags/docker/"}]},{"title":"Part 2: CHAPTER 9 Consistency and Consensus","slug":"Part-2-CHAPTER-9-Consistency-and-Consensus","date":"2019-12-12T06:35:04.000Z","updated":"2022-11-25T13:55:35.559Z","comments":true,"path":"2019/12/12/Part-2-CHAPTER-9-Consistency-and-Consensus/","link":"","permalink":"http://example.com/2019/12/12/Part-2-CHAPTER-9-Consistency-and-Consensus/","excerpt":"","text":"? It turns out that there are deep connections between ordering, linearizability, and consensus. 阐明它们之间的关系? 相比于单机事务,分布式事务有何不同?如何实现? 了解 google spanner(事务) 和 zk(consensus) 的实现 CAP 定理的理解 Summary http://www.bailis.org/blog/linearizability-versus-serializability ConsistentLinearizability: 提供 stronger consistency, make replicated data appear as though there were only a single copy, and to make all operations act on it atomically 优点: 易于理解 缺点: 对网络问题敏感,性能慢 Causality: which puts all operations in a single, totally ordered timeline, causality provides us with a weaker consistency model 优点: 对网络问题不敏感 使用场景受限 Consensus Consensus algorithms are a huge breakthrough for distributed systems: they bring concrete safety properties (agreement, integrity, and validity) to systems where everything else is uncertain, and they nevertheless remain fault-tolerant (able to make progress as long as a majority of nodes are working and reachable). They provide total order broadcast, and therefore they can also implement linearizable atomic operations in a fault-tolerant way. 定义: Deciding something in such a way that all nodes agree on what was decided, and such that the decision is irrevocable. 使用场景: Linearizable compare-and-set registers Atomic transaction commit Total order broadcast Locks and leases Membership&#x2F;coordination service Uniqueness constraint Zookeeper: providing an “outsourced” consensus failure detection membership service Consistency Guaranteess分布式 db 中,由于网络等因素,数据不一致一定会发生,因此 Most replicated databases provide at least eventual consistency。 eventual consistency: 所有 replicas 的数据最终会达到一致。 this is a very weak guarantee—it doesn’t say anything about when the replicas will converge 难以使用和测试: you need to be constantly aware of its limitations and not accidentally assume too much stronger consistency: 所有 replicas 数据总是保持一致 缺点: worse performance, less fault-tolerant 优势: easier to use correctly distributed consistency is mostly about coordinating the state of replicas in the face of delays and faults. Linearizability:3 个特质: Recency gurantee single operations on singel object time dependency and always move forward in time 定义: Linearizability(atomic consistency) is a guarantee about single operations on single objects. It provides a real-time (i.e., wall-clock) guarantee on the behavior of a set of single operations (often reads and writes) on a single object。 Linearizability is a recency guarantee(once a new value has been written or read, all subsequent reads see the value that was written, until it is overwritten again) on reads and writes of a register (an individual object). It doesn’t group operations together into transactions. Vs Serializability: linearizability can be viewed as a special case of strict serializability where transactions are restricted to consist of a single operation applied to a single object. [http://www.bailis.org/blog/linearizability-versus-serializability/] 使用场景: Locking and leader election: They use consensus algorithms to implement linearizable operations in a fault-tolerant way Constraints and uniqueness guarantees Cross-channel timing dependencies Implementing Linearizable SystemThe most common approach to making a system fault-tolerant is to use replication? 怎么说 Single-leader replication(potentially linearizable): If you make reads from the leader, or from synchronously updated followers, they have the potential to be linearizable. Consensus algorithms(linearizable): consensus protocols contain measures to prevent split brain and stale replicas. Multi-leader replication(not linearizable): because they concurrently process writes on multiple nodes and asynchronously replicate them to other nodes. Leaderless replication(probably not linearizable): sometimes claim that you can obtain “strong consistency” by requiring quorum reads and writes (w + r &gt; n) 简单的使用 quorums,即使满足 w + r &gt; n, 也不一定是 linearizable, 如: The Cost of LinearizabilityThe CAP theorem:一般来说，分区容错无法避免，因此可以认为 CAP 的 P 总是成立。CAP 定理告诉我们，剩下的 C 和 A 无法同时做到。 CAP 的定义比较局限,The CAP theorem as formally defined is of very narrow scope: it only considers one consistency model (namely linearizability) and one kind of fault (network partitions,vi or nodes that are alive but disconnected from each other). It doesn’t say anything about network delays, dead nodes, or other trade-offs. Linearizability and network delays:A faster algorithm for linearizability does not exist, but weaker consistency models can be much faster, so this trade-off is important for latency-sensitive systems. Ordering GuaranteesIt turns out that there are deep connections between ordering, linearizability, and consensus. Ordering and CausalityOrdering helps preserve causality: causal dependency: 比如事件的依赖顺序 happened before relationship Cross-channel timing dependencies … Causality imposes an ordering on events,These chains of causally dependent operations define the causal order in the system—i.e., what happened before what. The causal order is not a total orderCausality: 在 Casual 中没有依赖的操作可以并发执行,因此这些操作无法比较,所以 Casual 是 partial order(偏序关系). Linearizability: In a linearizable system, we have a total order of operations. Therefore, according to this definition, there are no concurrent operations in a linearizable datastore. Linearizability is stronger than causal consistencyLinearizability implies causality is what makes linearizable systems simple to understand and appealing. Causal consistency is the strongest possible consistency model that does not slow down due to network delays, and remains available in the face of network failures. Capturing causal dependenciesIn order to maintain causality, you need to know which operation happened before which other operation. In order to determine causal dependencies, we need some way of describing the “knowledge” of a node in the system. Causal consistency goes further: it needs to track causal dependencies across the entire database, not just for a single key In order to determine the causal ordering, the database needs to know which version of the data was read by the application Sequence Number Ordering使用 logical clock 给 event 编号,we can use sequence numbers or timestamps to order events, and they provide a total order. We can create equence numbers in a total order that is consistent with causality,先发生的 event 的 number 更小. Lamport timestamps: 保证 causality Each node has a unique identifier,and each node keeps a counter of the number of operations it has processed. The Lamport timestamp is then simply a pair of (counter, node ID). It provides total ordering. 原理解释: [https://jameshfisher.com/2017/02/12/what-are-lamport-timestamps/] [https://www.cnblogs.com/bangerlee/p/5448766.html] Total Order Broadcast This idea of knowing when your total order is finalized is captured in the topic of total order broadcast. Total order broadcast is usually described as a protocol for exchanging messages between nodes, Two safety properties always be satisfied: Reliable delivery: msg 需要被 delivered 到所有 node Messages are delivered to every node in the same order. This is no coincidence: it can be proved that a linearizable compare-and-set (or increment-and-get) register and total order broadcast are both equivalent to consensus. That is, if you can solve one of these problems, you can transform it into a solution for the others. This is quite a profound and surprising insight! Distributed Transactions and ConsensusAtomic commit,即保证分布式事务的原子性,需要依赖 consensus algo,2PC is a kind of consensus algorithm, which solving atomic commit. Atomic Commit and Two-Phase Commit(2PC)Atomicity prevents failed transactions from littering the database with half-finished results and half-updated state. From single-node to distributed atomic commit存储硬件层面保证: Thus, it is a single device (the controller of one particular disk drive, attached to one particular node) that makes the commit atomic. 节点层面保证: A node must only commit once it is certain that all other nodes in the transaction are also going to commit, A transaction commit must be irrevocable。 应用层面保证: However, from the database’s point of view this is a separate transaction, and thus any cross-transaction correctness requirements are the application’s problem Introduction to two-phase cmmitTwo-phase commit is an algorithm for achieving atomic transaction commit across multiple nodes—i.e., to ensure that either all nodes commit or all nodes abort. 在分布式系统里，每个节点都可以知晓自己操作的成功或者失败，却无法知道其他节点操作的成功或失败。当一个事务跨多个节点时，为了保持事务的原子性与一致性，需要引入一个协调(Coordinator)来统一掌控所有参与者(Participant)的操作结果，并指示它们是否要把操作结果进行真正的提交或者回滚. 2PC顾名思义分为两个阶段，其实施思路可概括为： 投票阶段(voting phase): 参与者将操作结果通知协调者； 提交阶段(commit phase): 收到参与者的通知后，协调者再向参与者发出通知，根据反馈情况决定各参与者是否要提交还是回. Much of the performance cost inherent in two-phase commit is due to the additional disk forcing (fsync) that is required for crash recovery, and the additional network round-trips. Coordinator failure2PC can become stuck waiting for the coordinator to recover. The only way 2PC can complete is by waiting for the coordinator to recover. This is why the coordinator must write its commit or abort decision to a transaction log on disk before sending commit or abort requests to participants:when the coordinator recovers, it determines the status of all in-doubt transactions by reading its transaction log. Any transactions that don’t have a commit record in the coordinator’s log are aborted. Thus, the commit point of 2PC comes down to a regular single-node atomic commit on the coordinator. Distributed Transactions in PracticeTwo types of distributed transactions are often conflated: Database-internal distributed transactions: work well as usual Heterogeneous distributed transactions: more challenge Exactly-once message processingThus, by atomically committing the message and the side effects of its processing, we can ensure that the message is effectively processed exactly once, even if it required a few retries before it succeeded. Such a distributed transaction is only possible if all systems affected by the transaction are able to use the same atomic commit protocol. XA transaction The transaction coordinator implements the XA API. Holding locks while in doubtThe database cannot release those locks until the transaction commits or aborts, Therefore, when using two-phase commit, a transaction must hold onto the locks throughout the time it is in doubt. This can cause large parts of your application to become unavailable until the in-doubt transaction is resolved. Recovering from coordinator failureOrphaned in-doubt transactions(如transaction log lost) cannot be resolved automatically, so they sit forever in the database, holding locks and blocking other transactions,只能通过管理员手动解决. Many XA implementations have an emergency escape hatch called heuristic decisions: allowing a participant to unilaterally decide to abort or commit an in-doubt transaction without a definitive decision from the coordinator. Limitations of distributed transactions对于 XA transactions, the key realization is that the transaction coordinator is itself a kind of database (in which transaction outcomes are stored), and so it needs to be approached with the same care as any other important database. Fault-Tolerant Consensus思想: Everyone decides on the same outcome, and once you have decided, you cannot change your mind, a consensus algorithm must satisfy the following properties: Uniform agreement: No two nodes decide differently Integrity: No node decides twice. Validity Termination: the idea of fault tolerance: 当有 node crash,也可以达成决策 Fault Tolerance: 为保证 termination,需要假设一旦 node crash, it suddenly disappears and never comes back, 以避免无限等待 node recover. 为保证 termination, 可以使用 quorum 来允许部分 node crash. 系统一定保证 safety properties,所以 Termination 不满足(如大量 node crash)也不会使系统做出错误的决策。 Consensus algorithm and total order broadcastConsensu algorithm: vsr, paxos, raft, zab… 这些算法不直接使用上述 Consensus 模型,而是 they decide on a sequence of values, which makes them total order broadcast algorithms. So, total order broadcast is equivalent to repeated rounds of consensus (each consensus decision corresponding to one message delivery). 例子: [https://www.cnblogs.com/j-well/p/7061091.html] 如何选举 leader:Consensus algo 使用 leader,但是不保证 leader 唯一,所以需要解决选主问题: 当有多个 leader 时候,use a leader in some form or another, but they don’t guarantee that the leader is unique. Node 为了确定自己是 leader,每次操作前需要举行投票确认自己的身份,因此 we have two rounds of voting: once to choose a leader, and a second time to vote on a leader’s proposal. Consensus algorithms define a recovery process by which nodes can get into a consistent state after a new leader is elected, ensuring that the safety properties are always met. Limitations of consensus 每次 proposal votes is a kind of synchronous replication. 影响性能 因为要进行 majority votes,所以对机器数量有要求(如:the remaining two out of three form a majority),如果发生 split brain,部分机器就会变得不可用。 Most consensus algorithms assume a fixed set of nodes that participate in voting, which means that you can’t just add or remove nodes in the cluster. Consensus systems generally rely on timeouts to detect failed nodes. 在网络不好时候可能导致频繁的选主. Sometimes, consensus algorithms are particularly sensitive to network problems. Membership and Coordination ServicesZooKeeper 使用场景: Linearizable atomic operations total order broadcast Failure detection Change notifications","categories":[{"name":"大数据","slug":"大数据","permalink":"http://example.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"ddia","slug":"ddia","permalink":"http://example.com/tags/ddia/"}]},{"title":"Part 2: CHAPTER 8 The Trouble with Distributed Systems","slug":"Part-2-CHAPTER-8-The-Trouble-with-Distributed-Systems","date":"2019-11-26T06:33:34.000Z","updated":"2022-11-25T13:49:57.028Z","comments":true,"path":"2019/11/26/Part-2-CHAPTER-8-The-Trouble-with-Distributed-Systems/","link":"","permalink":"http://example.com/2019/11/26/Part-2-CHAPTER-8-The-Trouble-with-Distributed-Systems/","excerpt":"","text":"? what degree they are avoidable how to think about the state of a distributed system how to reason about things that have happened Summary不同于单机,分布式系统中问题是无法避免的,本节主要探讨分布式系统中会遇到的以下问题: network 问题,比如网络不通导致节点之间的通信问题 clock 问题,如:不同 node 的 clock 不同步 process 问题,如: process may pause for a substantial amount of time at any point in its execution Partial failure: In a distributed system, there may well be some parts of the system that are broken in some unpredictable way, even though other parts of the system are working fine. partial failure 是不可预测的,This nondeterminism and possibility of partial failures is what makes distributed systems hard to work with。 单机和 supercomputer 面对 partial failure 时简单让整个系统崩溃重启就行,这是由于应用场景和硬件设计决定的。对于分布式系统,通常由许多跨地域的普通机器通过以太网进行组织,并为用户提供线上服务,因此出现错误时候简单的重启系统是不可接受的。所以 If we want to make distributed systems work, we must accept the possibility of partial failure and build fault-tolerance mechanisms into the software. 分布式系统中 partial failure 无法避免,因此需要 fault-tolerance 机制 Unreliable Networks 分布式系统通常采用 shared-nothing 架构, node 之间只能通过网络通信。通常使用的网络是 asynchronous packet networks(如因特网,以太网). Async network 无法保证数据交付和交付时效,所以会导致以下问题: request lost request 延迟 The remote node may have failed or 无法服务。 response lose response 延迟 The usual way of handling this issue is a timeout. 在实际使用中,由于系统和人为因素,网络问题总是无法避免。However, you do need to know how your software reacts to network problems and ensure that the system can recover from them. It may make sense to deliberately trigger network problems and test the system’s response. Detecting FaultsMany systems need to automatically detect faulty nodes。 In some specific circumstances you might get some feedback to explicitly tell you that something is not working If you want to be sure that a request was successful, you need a positive response from the application itself but in general you have to assume that you will get no response at all. You can retry a few times Timeouts and Unbounded Delays如果以 timout 作为 fail 机制,Timeout 如何选取,过长过短都会有问题。 如果系统保证 request 和 reponse 的最大时间,那么可以根据这些设置 timeout, 可惜多数系统并不做这种保证。 Prematurely declaring a node dead is problematic: 2d + r would be a reasonable timeout to use. 很多系统对 request 交付和 response 的最大时间是不做保证的。 For failure detection, it’s not sufficient for the system to be fast most of the time: if your timeout is low, it only takes a transient spike in round-trip times to throw the system off-balance. Network congestion(拥塞) and queueingSimilarly, the variability of packet delays on computer networks is most often due to queueing: 比如大量请求一个 node 导致网络拥塞,交换器在队列满后会丢弃后续的数据,这需要 resent request. request packet 到 os 时,如果所有 cpu busy, packet 需要排队。 In virtualized environments TCP performs flow control: This means additional queueing at the sender before the data even enters the network. In public clouds and multi-tenant datacenters,因为 resource 都是共享的,所以资源缺乏管理的时候,就容易发生拥塞 In such environments, you can only choose timeouts experimentally: Even better, rather than using configured constant timeouts, systems can continually measure response times and their variability (jitter), and automatically adjust timeouts according to the observed response time distribution. Synchronous(电路交换) Versus Asynchronous(分组交换) Networks电路交换: 如传统电话线路,通信时端到端建立连接,并且独占带宽资源。好处:可以建立可靠的,保证最大递交延迟的通信链路坏处:由于带宽独占,资源被静态切分导致资源利用率不高 分组交换:如 Ethernet 和 IP 网,带宽资源动态分配好处:更高的资源利用率坏处:会带来拥塞问题 而分布式应用通过 ip 网络通信,所以 queuing 问题无法避免。所以:Consequently, there’s no “correct” value for timeouts they need to be determined experimentally. Ethernet and IP are packet-switched protocols, which suffer from queueing and thus unbounded delays in the network. 所以无法构筑 establish a guaranteed maximum round-trip time 网络. Why do datacenter networks and the internet use packet switching? The answer is that they are optimized for bursty traffic. 使用因特网,doesn’t have any particular bandwidth requirement we just want it to complete as quickly as possible. TCP dynamically adapts the rate of data transfer to the available network capacity. With careful use of quality of service (QoS, prioritization and scheduling of packets) and admission control (rate-limiting senders), it is possible to emulate circuit switching on packet networks, or provide statistically bounded delay. Unreliable Clocks还是网络问题导致的通信时延让 clock 问题变得棘手。由于每个 node 上 clock 的误差,所以 Time-of-Day Clock 和 monotonic clock 都无法用来标识 event 发生顺序,需要使用 logical clock. It is possible to synchronize clocks to some degree: the most commonly used mechanism is the Network Time Protocol (NTP), which allows the computer clock to be adjusted according to the time reported by a group of servers. Monotonic Versus Time-of-Day ClocksTime-of-day clocksit returns the current date and time according to some calendar time-of-day 使用 ntp 进行时间同步,当其和 ntp server 不一致时,会强制重置本地 local clock, These jumps, as well as the fact that they often ignore leap seconds, make time-of-day clocks unsuitable for measuring elapsed time. Monotonic clocks A monotonic clock is suitable for measuring a duration (time interval), The name comes from the fact that they are guaranteed to always move forward (whereas a time-of-day clock may jump back in time). the absolute value of the clock is meaningless. 因为 monotonic clocks 不存在在不同节点之间同步的问题,所以在分布式系统使用其来衡量 elaspsed time 是很好的. Clock Synchronization And Accuracyhardware clocks and NTP can be fickle beasts If your NTP daemon is misconfigured, or a firewall is blocking NTP traffic, the clock error due to drift can quickly become large. Relying on Synchronized ClocksRobust software needs to be prepared to deal with incorrect clocks. Part of the problem is that incorrect clocks easily go unnoticed Thus, if you use software that requires synchronized clocks, it is essential that you also carefully monitor the clock offsets between all the machines. Timestamps for ordering events由于网络问题等,各个 node 上的 physical lock 之间存在差值,所以 分布式环境下使用 physical lock(time-of-day and monotonic clocks) 来标识 event 的发生顺序是无法保证正确性的。所以需要引入 logical clocks which are based on incrementing counters,are a safer alternative for ordering events Clock readings have a confidence interval通过 time-of-day clock 获取的时间往往在一个误差范围内 Thus, it doesn’t make sense to think of a clock reading as a point in time—it is more like a range of times。The uncertainty bound can be calculated based on your time source. Synchronized clocks for global snapshotsThe most common implementation of snapshot isolation requires a monotonically increasing transaction ID. 在分布式环境下实现 snapshot isolation 时,由于 clock 之间的不一致,实现一个 monotonically increasing transaction ID 比较困难,甚至 With lots of small, rapid transactions, creating transaction IDs in a distributed system becomes an untenable bottleneck. 在 google spanner 中,使用 confidence time interval 来实现 monotonically increasing transaction ID. In order to keep the wait time as short as possible, Spanner needs to keep the clock uncertainty as small as possible; for this purpose, Google deploys a GPS receiver or atomic clock in each datacenter, allowing clocks to be synchronized to within about 7 ms google spanner 怎么实现的? Process Pauses在分布式环境中, thread 可能会 pause so long, 比如 gc。由于在分布式系统中 has no shared memory, only messages sent over an unreliable network,所以单机中使用的并发机制如信号量,锁也都无法使用。 因此 A node in a distributed system must assume that its execution can be paused for a significant length of time at any point, even in the middle of a function Response time gurantees(怎么解决?) 在一些要求实时的系统中, there is a specified deadline by which the software must respond; if it doesn’t meet the deadline, that may cause a failure of the entire system. These are so-called hard real-time systems. 实现实时系统: Providing real-time guarantees in a system requires support from all levels of the software stack。For these reasons, developing real-time systems is very expensive, and they are most commonly used in safety-critical embedded devices. Moreover, “real-time” is not the same as “high-performance”—in fact, real-time systems may have lower throughput, since they have to prioritize timely responses above all else 所以: For most server-side data processing systems, real-time guarantees are simply not economical or appropriate. Consequently, these systems must suffer the pauses and clock instability that come from operating in a non-real-time environment. Limiting the impact of gcAn emerging idea is to treat GC pauses like brief planned outages of a node, and to let other nodes handle requests from clients while one node is collecting its garbage. This trick hides GC pauses from clients and reduces the high percentiles of response time. Some latency-sensitive financial trading systems use this approach. A variant of this idea is to use the garbage collector only for short-lived objects (which are fast to collect) and to restart processes periodically, before they accumulate enough long-lived objects to require a full GC of long-lived objects These measures cannot fully prevent garbage collection pauses, but they can usefully reduce their impact on the application. Knowledge, Truth, and Lies which distributed systems are different from programs running on a single computer: there is no shared memory, only message passing via an unreliable network with variable delays, and the systems may suffer from partial failures, unreliable clocks, and processing pauses. A node in the network cannot know anything for sure—it can only make guesses based on the messages it receives (or doesn’t receive) via the network. A node can only find out what state another node is in (what data it has stored, whether it is correctly functioning, etc.) by exchanging messages with it. In a distributed system, we can state the assumptions we are making about the behavior (the system model) and design the actual system in such a way that it meets those assumptions。 由于分布式系统中,节点只能依赖网络传递消息来进行状态判断,导致网络本身的问题和系统的问题无法区分。鉴于底层(os,network)等的不可靠,只能假设这些不可靠因素存在,在上层构筑可靠的系统。 The Truth Is Defined by the Majority分布式中,单个节点不可信,所以要依赖多个节点进行仲裁 A distributed system cannot exclusively rely on a single node, because a node may fail at any time, potentially leaving the system stuck and unable to recover. Instead, many distributed algorithms rely on a quorum, that is, voting among the nodes. 仲裁策略:The individual node must abide by the quorum decision and step down.A majority quorum allows the system to continue working if individual nodes have failed The leader and the clockFrequently, a system requires there to be only one of some thing.使用仲裁,这些 only one 也不一定可以被保证。如 the chosen one 问题: If a node continues acting as the chosen one, even though the majority of nodes have declared it dead, it could cause problems in a system that is not carefully designed. Fencing tokens使用 fencing 技术来解决 the chosen one 问题. Note that this mechanism requires the resource itself to take an active role in checking tokens by rejecting any writes with an older token than one that has already been processed—it is not sufficient to rely on clients checking their lock status themselves. In server side, it is a good idea for any service to protect itself from accidentally abusive clients. Byzantiine Faults 拜占庭将军问题实际上指的是在一个有 n 个节点的集群内，有 t 个节点可能发生任意错误的情况下，如果 n &lt;&#x3D; 3t，一个正确的 consensus 不可能达成假设节点总数是N，叛徒将军数为F，则当 N &gt;&#x3D; 3F+1 时，问题才有解，共识才能达成，这就是Byzantine Fault Tolerant（BFT）算法。 在仲裁的时候,如果 node lie 那么就会有拜占庭将军问题,在分布式系统中,所有 node 都是自己设置的,可以不必考虑拜占庭问题,在区块链等对等网络中需要考虑。 Although we assume that nodes are generally honest, it can be worth adding mechanisms to software that guard against weak forms of “lying”, for example, invalid messages due to hardware issues, software bugs, and misconfiguration. System Model and Reality system model, which is an abstraction that describes what things an algorithm may assume. To timing assumptions model: Synchronous model: it just means you know that network delay, pauses, and clock drift will never exceed some fixed upper bound. 由于现实中 unbounded delays 无法避免,所以这个 model 不切实际。 Partially synchronous model: Partial synchrony means that a system behaves like a synchronous system most of the time, but it sometimes exceeds the bounds for network delay, process pauses, and clock drift。符合实际情况 Asynchronous model: In this model, an algorithm is not allowed to make any timing assumptions—in fact, it does not even have a clock (so it cannot use timeouts). Node Failure model: Crash-stop faults: 只有 crash(he node may suddenly stop responding at any moment) 导致 node fail, crash 后无法恢复 Crash-recorvery faults: node crash 后可以通过 stable storage 存储的数据恢复,而内存中的数据会丢失 Byzantine (arbitrary) faults crash-recovery faults is generally the most useful model. Correctness of an algorithmsTo define what it means for an algorithm to be correct, we can describe its properties. An algorithm is correct in some system model if it always satisfies its properties in all situations that we assume may occur in that system mode 但是最坏的情况下,如 all nodes crash 或者 all network delay infinitely long,算法将失效 Safety and livenessTo clarify the situation, it is worth distinguishing between two different kinds of properties: safety and liveness propertiesAn advantage of distinguishing between safety and liveness properties is that it helps us deal with difficult system models, with liveness properties we are allowed to make caveats Safety: something bad will never happenLiveness: something good will must happen (but we don’t know when) https://zhuanlan.zhihu.com/p/37864854 Mapping system models to the real worldThe system model is a simplified abstraction of reality,理想很丰满,现实很骨感,算法模型做了一些假设,但是现实中这些假设会被打破,还是需要做容错处理 Model 的作用(推断证明): They are incredibly helpful for distilling down the complexity of real systems to a manageable set of faults that we can reason about, so that we can understand the problem and try to solve it systematically. Theoretical analysis and empirical testing are equally important.","categories":[{"name":"大数据","slug":"大数据","permalink":"http://example.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"ddia","slug":"ddia","permalink":"http://example.com/tags/ddia/"}]},{"title":"Part 2: CHAPTER 7 Transactions","slug":"Part-2-CHAPTER-7-Transactions","date":"2019-11-17T06:32:47.000Z","updated":"2022-11-25T13:50:36.704Z","comments":true,"path":"2019/11/17/Part-2-CHAPTER-7-Transactions/","link":"","permalink":"http://example.com/2019/11/17/Part-2-CHAPTER-7-Transactions/","excerpt":"","text":"Reference http://www.zhai14.com/blog/strenghen-comprehension-on-dirty-read-and-phantom.html https://www.aneasystone.com/archives/2017/10/solving-dead-locks-one.html Overview A transaction is a way for an application to group several reads and writes together into a logical unit. Conceptually, all the reads and writes in a transaction are executed as one operation: either the entire transaction succeeds (commit) or it fails (abort, rollback) Transaction 简化了应用对数据库的访问,使 error handling 变得简单,因为无需考虑 partial failure。 Widely used isolation levels(解决数据库的并发读写问题): read committed snapshot isolation(repeatable read) serialization Race conditions: Dirty reads: 一个 client 读取了另一个 client 尚未提交的写。 Dirty writes: 而当事务 A 更新时，事务 A 还没提交，事务 B 就也过来进行更新，覆盖了事务 A 提交的更新数据。Almost all transaction implementations prevent dirty writes. Read skew(Repeatable Read): A client sees different parts of the database at different points in time。snapshot isolation level(MVCC) 可以防止 read skew。 Lost updates: read-modify-write cycle 下容易发生,snapshot isolation level 可以防止该问题。 Write skew: Only serializable isolation prevents this anomaly. Phantom reads: Snapshot isolation prevents straightforward phantom reads, but phantoms in the context of write skew require special treatment, such as index-range locks. only serializable isolation protects against all of these issues. We discussed three different approaches to implementing serializable transactions: Literally executing transactions in a serial order: 如果您可以使每个事务的执行速度非常快，并且事务吞吐量足够低，可以在单个CPU核心上处理，那么这是一个简单而有效的选项。 Two-phase locking Serializable snapshot isolation (SSI):It uses an optimistic approach, allowing transactions to proceed without blocking. The Slippery Concept of a TransactionNosql 牺牲一致性来保证可用性; Relation db 使用 transaction 来保证强一致性。说到底么有孰优孰劣,都是根据场景的 trade-off。 The meaning of ACIDAtomicity在 ACID 语境下, Atomicity 不是关于并发的原子性,而是: The ability to abort a transaction on error and have all writes from that transaction discarded is the defining feature of ACID atomicity. Consistency this idea of consistency depends on the application’s notion of invariants, and it’s the application’s responsibility to define its transactions correctly so that they preserve consistency。 指的是应用程序(不是数据库)需要保证数据读写保证其应用程序的约束条件,以保证一致性。 Isolatoin(处理 concurrency problems)Isolation in the sense of ACID means that concurrently executing transactions are isolated from each other: they cannot step on each other’s toes. DurabilityDurability is the promise that once a transaction has committed successfully, any data it has written will not be forgotten, even if there is a hardware fault or the database crashes. Single-Object and Multi-Object OperationsTo recap, in ACID, atomicity and isolation describe what the database should do if a client makes several writes within the same transaction. Single-object writes 单条数据的读写也面临容错问题,比如当一条 10K 的 json 写入一半时网络断掉,所以对于 single-object, 存储也要提供 atomicity(可以用 log 实现) 和 isolation(可以用加锁的方式实现)。 The need for multi-object transactions single-object 事物无法满足所有的场景,如更新数据的时候需同步更新二级索引,所以 multi-object transaction 是需要的。 Handling errors and aborts Retrying an aborted transaction is a simple and effective error handling mechanism, 但是也会导致很多问题,比如不断重试对 server 造成压力。 Weak Isolation LevelsSerializable isolation means that the database guarantees that transactions have the same effect as if they ran serially (i.e., one at a time, without any concurrency) 但是 Serializable isolation has a performance cost, 所以 It’s therefore common for systems to use weaker levels of isolation, which protect against some concurrency issues, but not all。 Read Committed保证: no dirty reads: 脏读会读取到不一致的数据 no dirty writes: usually by delaying the second write until the first write’s transaction has committed or aborted. 实现:使用 row-level lock 实现,当一个 transaction 修改一个 object(row or document) 时,必须先获取这个 object 的锁直到 transaction 完成才释放。在 transaction 期间,其他 transaction 会读取到旧值。 Snapshot Isolation and Repeatable Read使用 Snapshot Isolation 来解决 repeatable read 的问题。 The idea is that each transaction reads from a consistent snapshot of the database,Snapshot isolation is a boon for long-running, read-only queries such as backups and analytics. 实现: multi-version concurrency control (MVCC): https://www.cnblogs.com/luchangyou/p/11321607.html 写: use write locks to prevent dirty writes 读: a key principle of snapshot isolation is readers never block writers, and writers never block readers。所以 the database must potentially keep several different committed versions of an object 来实现 snapshot 的效果。 Visibility rules for observing a consistent snapshotPut another way, an object is visible if both of the following conditions are true: At the time when the reader’s transaction started, the transaction that created the object had already committed. The object is not marked for deletion, or if it is, the transaction that requested deletion had not yet committed at the time when the reader’s transaction started. Indexes and snapshot isolation have the index simply point to all versions of an object and require an index query to filter out any object versions that are not visible to the current transaction use an append-only&#x2F;copy-on-write variant that does not overwrite pages of the tree when they are updated, but instead creates a new copy of each modified page. With append-only B-trees, every write transaction (or batch of transactions) creates a new B-tree root, and a particular root is a consistent snapshot of the database at the point in time when it was created. Preventing Lost UpdatesAtomic write operations实现: 一种方法是使用排他锁,taking an exclusive lock on the object when it is read so that no other transaction can read it until the update has been applied. 一种方式是 simply force all atomic operations to be executed on a single thread. Explicit locking如果 db 提供的 atomic write operations 不足, 就需要在应用层面提供锁机制来解决并发写带来的问题。 Automatically detecting lost updatesAn alternative is to allow them to execute in parallel and, if the transaction manager detects a lost update, abort the transaction and force it to retry its read-modify-write cycle. An advantage of this approach is that databases can perform this check efficiently in conjunction with snapshot isolation. Compare-and-setThe purpose of this operation is to avoid lost updates by allowing an update to happen only if the value has not changed since you last read it。如: 12-- This may or may not be safe, depending on the database implementationUPDATE wiki_pages SET content = &#x27;new content&#x27; WHERE id = 1234 AND content = &#x27;old content&#x27;; Write Skew and Phantoms(幻读) This effect, where a write in one transaction changes the result of a search query in another transaction, is called a phantomWrite skew can occur if two transactions read the same objects, and then update some of those objects (different transactions may update different objects). 怎么解决? use a serializable isolation level probably to explicitly lock the rows that the transaction depends on materializing conflicts: it takes a phantom and turns it into a lock conflict on a concrete set of rows that exist in the database。 Serializability Serializable isolation: It guarantees that even though transactions may execute in parallel, the end result is the same as if they had executed one at a time, serially, without any concurrency 3 种实现方式: Literally executing transactions in a serial order Two-phase locking Optimistic concurrency control techniques such as serializable snapshot isolation Actual Serial ExecutionEncapsulating transactions in stored procedures在使用 single-threaded serial transaction 的 db 中,db 和 client 进行交互式的事务非常的低效,所以一般 the application must submit the entire transaction code to the database ahead of time, as a stored procedure. Serial execution 总结: Every transaction must be small and fast It is limited to use cases where the active dataset can fit in memory· Write throughput must be low enough to be handled on a single CPU core。 Cross-partition transactions are possible,但是很耗资源 Two-Phase Locking(2PL)In 2PL, writers don’t just block other writers; they also block readers and vice versa. Two Phase: first phase: (while the transaction is executing) is when the locks are acquired second phase: (at the end of the transaction) is when all the locks are released. Implementationhaving a lock on each object in the database. The lock can either be in shared mode or in exclusive mode. 读取的时候获取共享锁,写的时候获取排他锁。可以允许同时多个读,但是读会阻塞写,写会阻塞其他读和写。 PerformanceFor this reason, databases running 2PL can have quite unstable latencies, and they can be very slow at high percentiles if there is contention in the workload. Predicate locks(谓词锁)The key idea here is that a predicate lock applies even to objects that do not yet exist in the database, but which might be added in the future (phantoms). Index-range locks Serializable Snapshot Isolation(SSI)基于 snapshot isolation 技术, SSI adds an algorithm for detecting serialization conflicts among writes and determining which transactions to abort. It provides full serializability, but has only a small performance penalty compared to snapshot isolation. 缺陷: 当有大量事务访问 the same objects,可能会导致大量的事务 abort。优势: 当 contention between transactions is not too high 时,相比悲观并发技术会有更好的性能。而且 Like under snapshot isolation, writers don’t block readers, and vice versa。 Decisions based on an outdated premise2 种 db 检测 query result might have changed 的方式: Detecting stale MVCC reads:When the transaction wants to commit, the database checks whether any of the ignored writes have now been committed. If so, the transaction must be aborted. By avoiding unnecessary aborts, SSI preserves snapshot isolation’s support for long-running reads from a consistent snapshot. Detecting writes that affect prior reads:When a transaction writes to the database, it must look in the indexes for any other transactions that have recently read the affected data. it simply notifies the transactions that the data they read may no longer be up to date. PerformanceThe rate of aborts significantly affects the overall performance of SSI. so SSI requires that read-write transactions be fairly short (long-running read-only transactions may be okay). However, SSI is probably less sensitive to slow transactions than two-phase locking or serial execution.","categories":[{"name":"大数据","slug":"大数据","permalink":"http://example.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"ddia","slug":"ddia","permalink":"http://example.com/tags/ddia/"}]},{"title":"Part 2: CHAPTER 6 Partitioning","slug":"Part-2-CHAPTER-6-Partitioning","date":"2019-10-27T06:28:55.000Z","updated":"2022-11-25T13:51:01.766Z","comments":true,"path":"2019/10/27/Part-2-CHAPTER-6-Partitioning/","link":"","permalink":"http://example.com/2019/10/27/Part-2-CHAPTER-6-Partitioning/","excerpt":"","text":"SummaryPartition 可以结合 Replication, 每个 partition 可以有多个 Replication 以保持高可用,如图: Partition(sharding): partitioning a large dataset into smaller subsets, that each piece of data (each record, row, or document) belongs to exactly one partition.使用 partition 可以分散读写,可以横向扩展。 The main reason for wanting to partition data is scalability: 数据和写入都可以分散到多个机器上去,以避免 hot spots。数据的 split 要合理,避免数据倾斜,同时要考虑 node 上下线的问题。 2 种 partitioning 的方式: Key range partitioning:key are sorted,partition 中也按 key 有序存储 优点: 高效的 range query 缺点: 访问的 key 相对集中时候的热点问题 rebalance 策略: dynamically by splitting the range into two subranges when a partition gets too big. Hash partitioning: 将 key hash 到对应的 partition,key 是无的序,从而换取更均匀的数据分布 优点: 更均匀的数据分布 缺点: 低效的 range query rebalance 策略: common to create a fixed number of partitions in advance, to assign several partitions to each node, and to move entire partitions from one node to another when nodes are added or removed 可以混合 2 种 partition 方式: using one part of the key to identify the partition and another part for the sort order. Secondary index 的 2 种 partition 方式: Document-partitioned indexes (local indexes): 和主键存储在同一个 partition,所以更新时只需更新单个 partition, 但是读取时需要聚集多个 partition 的数据 Term-partitioned indexes (global indexes): 分散在各个分区中,更新的时候需要更新多个,但是读取的时候只需要读取一个 partition Partitionoing of Key-Value DataOur goal with partitioning is to spread the data and the query load evenly across nodes. 如果 partition 不能平均分配, 就会出现数据倾斜和 hot spot 问题。 Partitioning by Key RangeOne way of partitioning is to assign a continuous range of keys (from some minimum to some maximum) to each partition In order to distribute the data evenly, the partition boundaries need to adapt to the data. Within each partition, we can keep keys in sorted order (SSTables and LSM-Trees) 劣势: 如果频繁访问部分 key,会造成 hot spots 问题。 Partitioning by Hash of Key使用 hash function 对 key 求 hash 值,you can assign each partition a range of hashes (rather than a range of keys), and every key whose hash falls within a partition’s range will be stored in that partition。如图: 优势: This technique is good at distributing keys fairly among the partitions. The partition boundaries can be evenly spaced. 劣势: range query 需要读取多个 partitoin,相比于 key range parition,比较低效。 Partitioning and Secondary Indexes A secondary index usually doesn’t identify a record uniquely but rather is a way of searching for occurrences of a particular value. 二级索引在关系型数据库中比较普遍。由于实现的复杂性,许多 k,v 存储(如 hbase)避免使用二级索引。搜索引擎(如 es)使用二级索引。 The problem with secondary indexes is that they don’t map neatly to partitions. There are two main approaches to partitioning a database with secondary indexes: document-based partitioning and term-based partitioning. Partitioning Secondary Indexes by Document(local index)In this indexing approach, each partition is completely separate: each partition maintains its own secondary indexes, covering only the documents in that partition.如图: 优势: 实现简单,写入和更新 index 比较简单高效。劣势: 读取成本高,需要读取所有 partition, Even if you query the partitions in parallel. Partitioning Secondary Indexes by Term(global index) Term: 来自全文索引的概念,这里指的是: where the terms are all the words that occur in a document. global index that covers data in all partitions,如果所有 index 都存储在一台 node 会成为瓶颈,所以 index 也需要 partitioned,可以通过 term 自身(方便 range scan)或者 hash 的方式(可以均匀分散)对 term index 进行 partition。 如图: 好处: 使用索引可以只读取相应的 partition, 提交读取效率。坏处: 因为写入的时候需要更新多个 partition 中的 index,所以写效率降低且复杂。 In practice, updates to global secondary indexes are often asynchronous, 因此 index 写入后不是立即可见的, 所以需要 distributed transactioon 来保证所有写入 partition 的索引的正确和立即可见。 Rebalancing Partitions The process of moving load from one node in the cluster to another is called rebalancing. Rebalancing 需满足: 将 load(存储,读写 load)公平的分配到集群的机器上。 Rebalancing 的同时, db 可以继续提供读写服务 要降低网络和磁盘 io,在 node 间只移动必要的数据。 Strategies for RebalancingWe need an approach that doesn’t move data around more than necessary. hash mod NThe problem with the mod N approach is that if the number of nodes N changes, most of the keys will need to be moved from one node to another Fixed number of partitions使用固定数量的 partitions is operationally simpler,然而选取合适的 partition 数量是非常困难的。而且在增减机器时候也有不必要的数据移动,如图: 如图: Dynamic partitioning当 partiton 中数据量增长到一个阈值的时候,可以 split 为 2 个partiton;同时,如果有数据删除,partiiton 变小,可以合并相邻 partition。 优势: the number of partitions adapts to the total data volume 缺陷: 当数据量小的时候被分配到一个 partition 中,开始所有的读写都会打到这个 partition, 为了缓解这个问题,可以预先分配一些 partition,但这需要 know what the key distribution is going to look like。 Partitioning proportionally to nodes在以上的 cases 中, the number of partitions is independent of the number of nodes. To have a fixed number of partitions per node, this approach also keeps the size of each partition fairly stable.这个方式可以很好的适应数据规模的变化,但是在增减机器 rebanlancing 的时候需要注意避免形成 unfailrly partition. Operations: Automatic or Manual Rebalancing自动 rebalancing 十分方便,但是是不可预测的,同时 automation can be dangerous in combination with automatic failure detection, 可能导致级联的错误。因此让人介入 rebalance 是比较好的。 Request Routingservice discovery 问题, client 如何知道要访问哪台机器? 有 3 种方式,如图: Many distributed data systems rely on a separate coordination service such as ZooKeeper to keep track of this cluster metadata,如图: Parallel Query ExecutionThe MPP(massively parallel processing) query optimizer breaks this complex query into a number of execution stages and partitions, many of which can be executed in parallel on different nodes of the database cluster。如各种分布式计算引擎: mr, spark, presto.","categories":[{"name":"大数据","slug":"大数据","permalink":"http://example.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"ddia","slug":"ddia","permalink":"http://example.com/tags/ddia/"}]},{"title":"Part 2: CHAPTER 5 Replication","slug":"Part-2-CHAPTER-5-Replication","date":"2019-09-29T06:27:13.000Z","updated":"2022-11-25T13:52:17.739Z","comments":true,"path":"2019/09/29/Part-2-CHAPTER-5-Replication/","link":"","permalink":"http://example.com/2019/09/29/Part-2-CHAPTER-5-Replication/","excerpt":"","text":"? Replication 和 partition 如何组合使用? 如何解决并发问题? 如何保证 consistency? 如何提高容错,应对如网络中断, latency spikes 问题 SummaryReplication 是什么?Each node that stores a copy of the database is called a replica. Replication 可以解决什么问题? High availability Disconnected operation: Allowing an application to continue working when there is a network interruption Latency: datacenter Scalability: 副本所提供的大量的读能力 主要面临的问题 容错和并发: It requires carefully thinking about concurrency and about all the things that can go wrong, and dealing with the consequences of those faults. 并发下的数据一致性 Three main approaches to replication: Single-leader replication: 简单,不需要处理数据冲突 Multi-leader replication Leaderless replication: 和 Multi-leader 容错更好,但是只能提供 very week consistency guarantees 如何解决容错和一致性? Replication can be synchronous or asynchronous,which has a profound effect on the system behavior when there is a fault. 比如在 lag increases 和 servers fail 时如何处理? 一些 consistency model which are helpful for deciding how an application should behave under replication lag: Read-after-write consistency Monotonic reads Consistent prefix reads Leaders and Followers(single-leader) 使用 leader-based replication(master–slave replication) 来解决 replication 写入问题的流程: 进行选主(elect leader) leader 写入时,会将数据以 replication log 或者 change stream 的形式发送给 followers, follower 更新本地 replication writes are only accepted on the leader (the followers are read-only from the client’s point of view). This mode of replication is a built-in feature of many relational databasesLeader-based replication is not restricted to only databases, 如消息队列也会使用 Syncchronous Versus Asynchronous ReplicationSynchronous优势(保证数据一致性):The follower is guaranteed to have an up-to-date copy of the data that is consistent with the leader. 劣势(系统可用性差):如果有 follower 无法响应(如遇到网络问题),那么 the write cannot be processed. The leader must block all writes and wait until the synchronous replica is available again. 所以所有 followers 都是同步是不现实的,一般使用如 semi-synchronous 方式,至少保证系统中一个 follower 是 sync 的,其他 follower 则是 async 的,This guarantees that you have an up-to-date copy of the data on at least two nodes Completely Asynchronous如何在并发场景下保证数据一致? 优势(可用性高): The leader can continue processing writes, even if all of its followers have fallen behind. 劣势: 写丢失: If the leader fails and is not recoverable, any writes that have not yet been replicated to followers are lost. 不保证写入: A write is not guaranteed to be durable, even if it has been confirmed to the client 尽管如此,全异步还是被广泛使用,尤其在 there are many followers or if they are geo‐graphically distributed. Setting Up New Followers(扩容时的一致性)set up 新 follower 时,如何保证 the new follower has an accurate copy of the leader’s data? Take a consistent snapshot of the leader’s database at some point in time—if possible, without taking a lock on the entire database. Copy the snapshot to the new follower node. The follower connects to the leader and requests all the data changes that have happened since the snapshot was taken. Handling Node Outages(容错)其实就是如何保证 High Availability: Thus, our goal is to keep the system as a whole running despite individual node failures, and to keep the impact of a node outage as small as possible. 所以如何在处理机器不可用问题,以保证高可用? Follower failure: Catch-up recoveryfollower 本地保存 data chanage log(received from leader), 可以从 log 中进行恢复, 然后再从 leader 拿到停止时间内的 log 进行重建 the follower can connect to the leader and request all the data changes that occurred during the time when the follower was disconnected Leader failure: Failover如何处理? 重新选主 followers 从新主获取数据 client 和新 leader 交互 ha 机制: Determining that the leader has failed Choosing a new leader:The best candidate for leadership is usually the replica with the most up-to-date data changes from the old leader (to minimize any data loss). Getting all the nodes to agree on a new leader is a consensus problem Reconfiguring the system to use the new leader:The system needs to ensure that the old leader becomes a follower and recognizes the new leader. 一些问题: 如果使用异步 replication, new leader 和 old leader 之间可能存在 write 数据的差异, 此时 common solution 是 old leader 丢弃这些 writes. 丢弃写入可能会导致和其他外部存储系统数据不一致 split brain(有多个 node 认为自己是 leader): data is likely to be lost or corrupted; you can end up with both nodes being shut down 合理的 leader 失效 timeout 设置? load spike, network glitch 这些情况需要被综合考虑 node failures; unreliable networks; trade-offs around replica consistency, durability, availability and latency are in fact fundamental problems in distributed systems Implementation of Replication Logsthe log is an append-only sequence of bytes containing all writes to the database leader 将 log 写到本地,同时发送给 followers, follower 执行 log 来建立和 leader 相同的 replication Statement-based replication最简单的情况:leader logs every write request(statement) that it executes and sends that statement log to its followers 缺点: nondeterministic function(like now()) is likely to generate a different value on each replica. they must be executed in exactly the same order on each replica, or else they may have a different effect. This can be limiting when there are multiple concurrently executing transactions. Statements that have side effects, unless the side effects are absolutely deterministic. 以上问题是可以绕过的 Write-ahead log(WAL) shipping这是什么? 如何实现? Describes the data on a very low level: a WAL contains details of which bytes were changed in which disk blocks。 因此难以适应数据存储格式的变化,但是可以解决 statement-based 的问题。 Logical(row-based) log replicationLogical log: 使用和 storage engine 不同的存储格式, 以便于解耦 log 和 storage engine 优点: 因为解耦所以 it can more easily be kept backward compatible, allowing the leader and the follower to run different versions of the database software, or even different storage engines. A logical log format is also easier for external applications to parse Trigger-based replicationmove replication up to the application layer. An alternative is to use features that are available in many relational databases: triggers and stored procedures. A trigger lets you register custom application code that is automatically executed when a data change (write transaction) occurs in a database system. The trigger has the opportunity to log this change into a separate table, from which it can be read by an external process. That external process can then apply any necessary application logic and replicate the data change to another system.it can nevertheless be useful due to its flexibility. 优点是灵活,缺点是 is more prone to bugs and limitations than the database’s built-in replication Problems with Replication Lag Replication Lag: the delay between a write happening on the leader and being reflected on a follower In this read-scaling architecture(Leader-based), this approach only realistically works with asynchronous replication, why?: synchronously 会导致系统不可用的几率变高 so a fully synchronous configuration would be very unreliable.如果使用异步系统,又会面临数据一致性问题。 所以本节主要介绍导致 lag 过长的场景和解决这些问题的方法. Reading Your Own Writes read-after-write consistency(read-your-writes consistency): 保证写入方写入后立即可见,但对其他 user 不进行保证 一些解决方案: When reading something that the user may have modified, read it from the leader; otherwise, read it from a follower, 但是当一个用户对系统进行大量读写的话,这个方式就会变得低效 client 端可以记录 write 的时间戳,向系统请求的时候带上时间戳,系统根据时间戳来判断当前 replica 是否可以提供服务,如果不行,就尝试其他的 replica, 否则一直等待直到收到数据写入,可以服务 If your replicas are distributed across multiple datacenters (for geographical proximity to users or for availability), there is additional complexity. Any request that needs to be served by the leader must be routed to the datacenter that contains the leader. 保证 Cross-device read-after-write consistency 的问题: remembering the timestamp of the user’s last update become more difficult If your replicas are distributed across different datacenters,there is no guarantee that connections from different devices will be routed to the same datacenter Monotonic Reads(async 情况下)对于 async followers 来说,由于 lag 不同,用户可能从不同的 follower 读到不同的数据,有些是过时的。 Monotonic reads 保证不读到过时的数据. It’s a lesser guarantee than strong consistency, but a stronger guarantee than eventual consistency. If one user makes several reads in sequence, they will not see time go backward One way of achieving monotonic reads is to make sure that each user always makes their reads from the same replica Consistent Prefix Readsconsistent prefix reads: This guarantee says that if a sequence of writes happens in a certain order, then anyone reading those writes will see them appear in the same order. This is a particular problem in partitioned (sharded) databases。in many distributed,databases, different partitions operate independently, so there is no global ordering of writes 一种解决方式事保证互相关联的写入被写入到同一个 partition. Solutions for Replication Lag在 eventually consisten system 中 lag 不断增长怎么办?provide a stronger guarantee, such as read-after-write 但是 Eventual consistency 是不够的, db 需要 Transactions 来提供强一致性保证。 Multi-Leader Replication可用性高,但是会面临 write conflicts 的问题,如何解决数据一致性问题? 是什么: Multi-leader configuration (also known as master–master or active&#x2F;active replication). In this setup, each leader simultaneously acts as a follower to the other leaders. 如 akka cluster Use Cases for Multi-Leader Replication不适应于创建 datacenter Multi-datacenter operation 相比于 single-leader 方案的优势: 去中心化,write request 分散到各个 leader, which means the perceived performance may be better. Tolerance of datacenter outages: 不需要重新选举 leader Tolerance of network problems: A multi-leader configuration with asynchronous replication can usually tolerate network problems better: a temporary network interruption does not prevent writes being processed 缺点: the same data may be concurrently modified in two different datacenters, and those write conflicts must be resolved 一些使用 multi-leader 的系统: Tungsten Replicator for MySQL BDR for PostgreSQL GoldenGate for Oracle Clients with offline operationif you have an application that needs to continue to work while it is disconnected from the internet. 怎么解决？ 比如在多个设备上使用同一个app, every device has a local database that acts as a leader (it accepts write requests), and there is an asynchronous multi-leader replication process (sync) between the replicas of your app on all of your devices CouchDB is designed for this mode of operation Collaborative editingReal-time collaborative editing applications allow several people to edit a document simultaneously 会有啥问题? 怎么解决? 如果加锁的话就 is equivalent to single-leader replication with transactions on the leader. 如果想避免枷锁,就会带来 challenges of multi-leader replication, including requiring conflict resolution Handling Write ConflictsThe biggest problem with multi-leader replication is that write conflicts can occur. 比如对同一份数据的修改请求到不同的 leader. 在 Multi-leader 情况下,只能异步的进行冲突检测,it may be too late to ask the user to resolve the conflict. Conflict avoidanceavoid them: if the application can ensure that all writes for a particular record go through the same leader, then conflicts cannot occur. Converging toward a consistent stateIn a multi-leader configuration, there is no defined ordering of writes, so it’s not clear what the final value should be. 所以 all replicas must arrive at the same final value when all changes have been replicated. 4 种常用解决 write conflict 的方法: Give each write a unique ID,比如 last write wins方式:冲突时,选择最新的一条,但是这会有丢数据的风险 Give each replica a unique ID,ID越高则其对应的数据也有更高优先级，不过也意味着某些数据的无条件丢失。 合并：将冲突的值排序并连接起来, 如“你很漂亮&#x2F;帅” Record the conflict in an explicit data structure that preserves all information, and write application code that resolves the conflict at some later time Custom conflict resolution logic用户编写冲突处理逻辑, That code may be executed on write or on read: On write: 修改数据时若检测到了冲突，则立即调用conflict handler处理。 On read: 存储所有有冲突的写入,读取时给出所有版本数据,用户自己进行数据选择。 Note that conflict resolution usually applies at the level of an individual row or document, not for an entire transaction Automatic conflict resolution could make multi-leader data synchronization much simpler for applications to deal with. Multi-Leader Replication Topologies各个拓扑的介绍和优缺点? replication topology: 如图,描述 writes request 在所有 nodes 间的传递轨迹。 Circular 和 star 的问题在于如果有 node down 掉, it can interrupt the flow of replication messages between other nodes. all-to-all 可以通过其他传递路径来避免这个问题. all-to-all 缺点: 网络速度差异会导致 the result that some replication messages may “overtake” others Leaderless ReplicationAllowing any replica to directly accept writes from clients. 写的时候发送 write 到多个 replica, 多数成功则成功。读的时候也从多个 replica 读取,通过版本对比采纳最新的那份数据。 In some leaderless implementations, the client directly sends its writes to several replicas, while in others, a coordinator node does this on behalf of the client. However, unlike a leader database, that coordinator does not enforce a particular ordering of writes Leaderless replication is also suitable for multi-datacenter operation, since it is designed to tolerate conflicting concurrent writes, network interruptions, and latency spikes. Writing to the Database When a Node Is Down不存在 failover, client 直接忽略写失败的 replica, 写会同步请求多个 node,Version numbers are used to determine which value is newer Read repair and anti-entropy当 down 掉的机器恢复后,如何恢复缺失的数据呢,有 2 种常用方式: Read repair: 读取的时候检测 stale value 并 recover, This approach works well for values that are frequently read. Anti-entropy process: some datastores have a background process that constantly looks for differences in the data between replicas and copies any missing data from one replica to another 没有顺序如何保证写入的因果关系? Quorums for reading and writing设想:n: replica 数量w: 每个 write 都要保证 w 个节点写成功r: 读取的时候至少从 r 个节点读取数据所以只要 w + r &gt; n, 这样读取的时候一定会读取到 w 中至少一个节点的数据,所以可以读到最新的数据。 A common choice is to make n an odd number (typically 3 or 5) and to set w &#x3D; r &#x3D; (n + 1) &#x2F; 2 (rounded up). 可概以根据应用的读写情况对参数做调整. The quorum condition, w + r &gt; n, allows the system to tolerate unavailable nodes as follows: Limitations of Quorum Consistency即使 w + r &gt; n, 也是无法完全保证数据强一致性的.Stronger guarantees generally require transactions or consensus. Sloppy Quorums and Hinted Handoff(草率的对话和暗示的交接)leader-less dababase 可以提供 high availability and low latency, and that can tolerate occasional stale reads. 有些 quorums 并不能提供良好的 fault-toleratn,比如当发生网络split 时,一些 client 会离开集群,一些新的 client 可能会加入集群.此时如果保证 w 写入,但是 read 可能会读取到新加入的 client,从而无法拿到最新的数据. Detecting Concurrent WritesThe problem is that events may arrive in a different order at different nodes, due to variable network delays and partial failures 当数据对写入顺序有要求(如聊微信)时,如何保证 eventually consisteny Last write wins(discarding concurrent writes)可以在多个冲突的写入中间选取最难 recent 的那个,如 LWW,其他的写入则被丢弃 The only safe way of using a database with LWW is to ensure that a key is only written once and thereafter treated as immutable, thus avoiding any concurrent updates to the same key. Capturing the happens-before relationship可以在 write request 中加入版本号,server 通过版本号来确定 2 个操作是不是并发(2个操作没有依赖关系)执行。 When a write includes the version number from a prior read, that tells us which previous state the write is based on. Merging concurrently written valuesAs merging siblings in application code is complex and error-prone, there are some efforts to design data structures that can perform this merging automatically。","categories":[{"name":"大数据","slug":"大数据","permalink":"http://example.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"ddia","slug":"ddia","permalink":"http://example.com/tags/ddia/"}]},{"title":"Part 1: CHAPTER 4 Encoding and Evolution","slug":"Part-1-CHAPTER-4-Encoding-and-Evolution","date":"2019-09-15T06:21:45.000Z","updated":"2022-11-25T13:52:54.260Z","comments":true,"path":"2019/09/15/Part-1-CHAPTER-4-Encoding-and-Evolution/","link":"","permalink":"http://example.com/2019/09/15/Part-1-CHAPTER-4-Encoding-and-Evolution/","excerpt":"","text":"? Encoding 的含义是什么? 如何实现? Encoding 需要注意什么? 兼容性 Encoding 的现状和发展趋势是什么? 如何保持 forward compatibility? Protocol Buffers, Thrift, and Avro 都是什么? REST, RPC, Actors, message queue 各自的概念和区别以及应用场景? RPC 相比于 REST 的优势在哪? 主要应用场景是什么? evolution of schemas over time? Summary data outlives code 本文介绍了多种 encoding(turning data structures into bytes on the network or bytes on disk) 方式,并介绍了它们的实现细节和效率。 涉及到 3 种 encoding formats:5 Programming language–specific: 被语言限制并且对兼容性支持不好 Textual formats: JSON, XML, and CSV: (1)兼容性取决于使用方式 (2)These formats are somewhat vague about datatypes Binary schema–driven formats: Thrift, Protocol Buffers, Avro 由于分布式环境下 many services need to support rolling upgrades, 所以 encoding 需要支持向前和向后兼容 encoding 的重要使用场景: Databases: 写入时 encode, 读取时 decode RPC and REST APIs: client encodes a request, the server decodes the request and encodes a response, and the client finally decodes the response Asynchronous message passing: encoded by the sender and decoded by the recipient usewe must assume that different nodes are running the different versions of our application’s code. Formats for Encoding Data使用特定语言的序列化和反序列化类库存在跨语言,兼容性,安全性,性能等诸多问题,不建议使用。所以着重介绍 binary encoding。 JSON, XML, CSV are textual formats适合作为数据交换 format, JSON,XML,CSV 的问题: There is a lot of ambiguity around the encoding of numbers don’t support binary strings There is optional schema support for both XML and JSON,and thus quite complicated to learn and implement CSV does not have any schema Binary encoding在大数据量下使用 textual formats(更具可读性) 太过耗费空间,所以需要 binary encoding。 Some of these formats extend the set of datatypes, they need to include all the object field names within the encoded data. Thrift and Protocol Buffers2 者都 based on the same principle,即: 需要定义数据的 schema, 字段可以被标示为 optional 或者 required, 可以在 runtime 时对字段合法性进行 check, 同时使用 field tag 而非 field name 来压缩数据。 每条数据中包含 schema 信息,参见下图 Protocol Buffers Thrift(not good fit for Hadoop) 有 2 种编码方式: BinaryProtocol 和 CompactProtocol Field tags and schema evolutionHow do Thrift and Protocol Buffers handle schema changes while keeping backward and forward compatibility? Forward compatibility: 添加了新字段后,老的代码读取数据的时候忽略新的字段即可 Backward compatibility: As long as each field has a unique tag number, new code can always read old data Removing a field is just like adding a field, with backward and forward compatibility concerns reversed. Datatypes and schema evolution数据类型改变时, 只能部分的支持兼容性。数据类型改变时, there is a risk that values will lose precision or get truncated Thrift 有 list 类型,不支持 single 和 list 的相互转换 protocol buffer 没有 list 类型,list 是 mutiple single, 所以可以支持 single 和 list 间的转换 Avro: hadoop 的子项目Avro also uses a schema to specify the structure of the data being encoded. encoding 数据中不包含 schema 信息,读取的时候 you go through the fields in the order that they appear in the schema and use the schema to tell you the datatype of each field. The writer’s schema and the reader’s schema Writer&#39;s schema: encodes the data using whatever version of the schema it knows aboutReader’s schema: it is expecting the data to be in some schema 那怎么保证 shcema 变更导致的兼容性? The key idea with Avro is that the writer’s schema and the reader’s schema don’t have to be the same—they only need to be compatible Schema evolution rulesTo maintain compatibility, you may only add or remove a field that has a default value Avro doesn’t have optional and required markers in the same way as Protocol Buffers and Thrift do. Changing the datatype of a field is possible, provided that Avro can convert the type. But whats the writers schemahow does the reader know the writer’s schema with which a particular piece of data was encoded? the writer of that file can just include the writer’s schema once at the beginning of the file The simplest solution is to include a version number at the beginning of every encoded record, and to keep a list of schema versions in your database (类似 es) 网络通信时: they can negotiate the schema version on connection setup and then use that schema for the lifetime of the connection. Dynamically generated schemasAvro schema 中不包含 tag number, 所以 Avro is friendlier to dynamically generated schemas. This kind of dynamically generated schema simply wasn’t a design goal of Thrift or Protocol Buffers, whereas it was for Avro. Modes of DataflowThere are many ways data can flow from one process to another, 看看 encoding 在其中的价值: Via databases Via service calls Via asynchronous message passing Dataflow Through Databases关系型中的数据写入时间差异很大,所以向前向后兼容都需要。 Dataflow Through Services: REST and RPCSOA(microservices): This approach is often used to decompose a large application into smaller services by area of functionality. In other words, we should expect old and new versions of servers and clients to be running at the same time, and so the data encoding used by servers and clients must be compatible across versions of the service API precisely what we’ve been talking about in this chapter. Web servicesREST: is not a protocol, but rather a design philosophy that builds upon the principles of HTTP. It emphasizes simple data formats, using URLs for identifying resources and using HTTP features for cache control, authentication, and content type negotiation. SOAP: is an XML-based protocol for making network API requests. Although it is most commonly used over HTTP, it aims to be independent from HTTP and avoids using most HTTP features. SOAP 的例子:https://www.cnblogs.com/mfrbuaa/p/3986739.html The problems with remote procedure calls(RPCs)Although RPC seems convenient at first, the approach is fundamentally flawed: 网络问题不可控,必须自己做容错 if you don’t get a response from the remote service, you have no way of knowing whether the request got through or not. 需要 deduplication 机制 A network request is much slower than a function call, and its latency is also wildly variable 序列化和反序列化 不同语言之间的类型转换可能导致问题 Thus, you only need backward compatibility on requests, and forward compatibility on responses. Message-Passing Dataflow(如: Akka)A client’s request (usually called a message) is delivered to another process with low latency. message is not sent via a direct network connection, but goes via an intermediary called a message broker (also called a message queue or message-oriented middleware), which stores the message temporarily. A sender normally doesn’t expect to receive a reply to its messages. This communication pattern is asynchronous: the sender doesn’t wait for the message to be delivered, but simply sends it and then forgets about it. Message brokersproduecers -&gt; topic -&gt; consumersMessage brokers typically don’t enforce any particular data model Distributed actor frameworksLocation transparency works better in the actor model than in RPC, because the actor model already assumes that messages may be lost. Akka uses Java’s built-in serialization by default, which does not provide forward or backward compatibility","categories":[{"name":"大数据","slug":"大数据","permalink":"http://example.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"ddia","slug":"ddia","permalink":"http://example.com/tags/ddia/"}]},{"title":"Part 1: CHAPTER 3 Storage and Retrieval","slug":"Part-1-CHAPTER-3-Storage-and-Retrieval","date":"2019-09-14T06:19:32.000Z","updated":"2022-11-25T13:53:18.688Z","comments":true,"path":"2019/09/14/Part-1-CHAPTER-3-Storage-and-Retrieval/","link":"","permalink":"http://example.com/2019/09/14/Part-1-CHAPTER-3-Storage-and-Retrieval/","excerpt":"","text":"? how databases arrange data on disk so that we can find it again efficiently? how we can store the data that we’re given(如何存储数据)? how we can find it again when we’re asked for it(如何检索数据)? storage engine optimized for transactional(OLTP) 和 optimized for analytics(OLAP) 有何不同? log-structured storage engines(如 LSM-Trees) and page-oriented storage enginers(如 B-Trees)? index 的类型和原理? 内存中的结构是什么样, 磁盘的结构又是什么样? 如何从内存刷到磁盘, 又从磁盘恢复到内存? data model, index, column-oriented, encoding 之间是如何配合的,它们之间的关系是什么 OLTP(online transaction processing) Overview A transaction needn’t necessarily have ACID (atomicity, consistency, isolation, and durability) properties. Transaction processing just means allowing clients to make low-latency reads and writes as opposed to batch processing jobs, which only run periodically (for example, once per day). Storage: 相比随机写入顺序写入简单高效, 因此 Many databases internally use a log(an append-only sequence of records), which is an append-only(the simplest possible write operation) data file. Retrieval: TP 应用通常面向用户,使用 key 检索数据,结果集也通常较小,对响应时间有要求,因此 store engine 使用 index 来加速查询。Disk seek time is often the bottleneck here. Index: Index 利用额外的存储(来自 data)来加速查询. 维持 index 会面临一些成本,尤其是当写入时,需要同步更新 index。由于 index 可以加速查询, 降低写入效率,所以索引的建立需开发人员去权衡. Hash Indexes with append-onlyStorage: 数据以 (k,v) pair 的形式存储在磁盘,以 append only 的方式进行追加Retrieval: 在内存中维护 hash index,数据追加时同步更新 index 以 append-only 方式追加会导致磁盘空间不足,为了解决这个问题: 将 log 切分为定长的 segment, 当 segment file 到达固定大小时,后续的 log 写入新的 segment file 定期对 segment file 进行 compaction(throwing away duplicate keys in the log, and keeping only the most recent update for each key),也可以将多个 segment file compaction 为一个 segment file 是只读的, compaction 会产生新的文件.在 compaction 的同时, old segment file 仍可以提供读写服务.当 compaction 完成时,使用新的 segment file, 删除老的 segment file。 这个方案一些要关注的点: File format: use a binary format that first encodes the length of a string in bytes, followed by the raw string Deleting records: use a binary format that first encodes the length of a string in bytes, followed by the raw string. merge 的时候会删除数据. Crash recovery: Bitcask speeds up recovery by storing a snapshot of each segment’s hash map on disk, which can be loaded into memory more quickly. Partially written records: include checksums, allowing such corrupted parts of the log to be detected and ignored. Concurrency control: 单线程写入,一个 file 要么是 append-only 要么是 immutable, 所以可以多线程读取 优势: append 和 segment merge 是顺序写入操作,比随机写入要高效 Concurrency and crash recovery are much simpler if segment files are append-only or immutable Merging old segments avoids the problem of data files getting fragmented over time. Limitations: The hash table must fit in memory,当 hash table 很大被迫放入磁盘时,索引效率就会下降,还会面临 hash 冲突的问题. Range queries are not efficient. SSTables(Sorted String Table) and LSM-Trees https://www.open-open.com/lib/view/open1424916275249.htmlhttps://ggaaooppeenngg.github.io/zh-CN/2017/03/31/%E4%BB%8E%E6%9C%B4%E7%B4%A0%E8%A7%A3%E9%87%8A%E5%87%BA%E5%8F%91%E8%A7%A3%E9%87%8Aleveldb%E7%9A%84%E8%AE%BE%E8%AE%A1/ 如何使用 SSTables 来解决 hash index 的问题呢 Storage: 在 hash index 中 k,v 数据按照写入顺序存储,且同一个 k 会存在多个 segment file 中。在 SSTable 中, we require that the sequence of key-value pairs is sorted by key.We also require that each key only appears once within each merged segment file Retrieval: 因为 key 按序存储,所以 index 中的一个 key 可以表示一个范围的上&#x2F;下界,找到数据所在范围后再 scan 者个范围内的数据。先检索内存中的 memtable, 再根据时间检索磁盘上的 SSTables. Advantages: Merging segments is simple and efficient, even if the files are bigger than the available memory, 类似于合并有序数组 you no longer need to keep an index of all the keys in memory. 因为有序, hash table 中的一个 key 可以代表一个范围的上&#x2F;下界,通过 scan 这个小范围来检索数据 it is possible to group those records into a block and compress it before writing it to disk. 这可以节省磁盘同时也能降低 IO 带宽 Constructing and maintaining SSTablesSorted structure on disk: LSM-tree(Log-Structured Merge-Tree)?Sorted structure in memory: red-black trees or AVL trees Work flow: add write to and in-memory balanced tree structure called memtable When the memtable gets bigger than some threshold—typically a few megabytes—write it out to disk as an SSTable file. In order to serve a read request, first try to find the key in the memtable, then in the most recent on-disk segment, then in the next-older segment, etc. From time to time, run a merging and compaction process in the background to combine segment files and to discard overwritten or deleted values. 唯一的问题: 当 database crashed, 还没刷到磁盘的 memetable 就会丢失,为了避免这个问题,写一条数据时,同时以简单的 append 方式追加到一个 log 中,便于 crash 后 memtable 的恢复 LSM-tree(Log-Structured Merge-Tree)写快读慢,因为顺序写入,读取的时候需要检索 memtable 和 sstable. Storage engines that are based on this principle of merging and compacting sorted files are often called LSM storage engines. the basic idea of LSM-trees—keeping a cascade of SSTables that are merged in the background is simple and effective 由于数据存储有序,所以 index 可以进行 range query,这保证了再大数据集下不错的检索性能。由于顺序写入, LSM-tree 也可以支持大的写吞吐; Reads are typically slower on LSM-trees because they have to check several different data structures and SSTables at different stages of compaction. Performance optimizations 检索一个不存在的 key 费时费力,为了解决这个问题,常使用额外的 bloom-filter Merge 和 compact 方式: size-tiered: newer and smaller SSTables are successively merged into older and larger SSTables. leveled compaction: the key range is split up into smaller SSTables and older data is moved into separate “levels,” which allows the compaction to proceed more incrementally and use less disk space B-TreesStorage: (k,v) 按 k 有序存储,which allows efficient key-value lookups and range queries。B-trees break the database down into fixed-size blocks or pages, traditionally 4 KB in size (sometimes bigger), and read or write one page at a time. This design corresponds more closely to the underlying hardware, as disks are also arranged in fixed-size blocks. Most databases can fit into a B-tree that is three or four levels deep, so you don’t need to follow many page references to find the page you are look‐ing for. (A four-level tree of 4 KB pages with a branching factor of 500 can store up to 256 TB.) Making B-trees reliableB-tree is to overwrite a page on disk with new data. It is assumed that the overwrite does not change the location of the page; fault-tolerant when database crash?In order to make the database resilient to crashes, it is common for B-tree implementations to include an additional data structure on disk: a write-ahead log (WAL, also known as a redo log). 并发问题: This is typically done by protecting the tree’s data structures with latches (lightweight locks). B-tree optimizations copy-on-write …… Comparing B-Trees and LSM-TreesB-tree: 写慢读快;每条数据只存在于一个地方,所以可以支持强大的事务能力Lsm-tree: 写快读慢,同一个 key 可能在不同的 segment 中 Advantages of LSM-trees相比于 B-trees, LSM-trees 可以维持 higher write throughput: lsm-trees have lower write amplification sequentially write compact SSTable LSM-trees 磁盘利用率相对较高: LSM-trees can be compressed better, and thus often produce smaller files on disk than B-trees. write amplification(写放大): A B-tree index must write every piece of data at least twice: index 和 数据 Log-structured indexes also rewrite data multiple times due to repeated compaction and merging of SSTables Downsides of LSM-trees:写入时空间换时间 compaction process 有时和 ongoing reads and writes process 会互相干扰 Another issue with compaction arises at high write throughput: the disk’s finite write bandwidth needs to be shared between the initial write (logging and flushing a memtable to disk) and the compaction threads running in the background. Other Indexing Structures二级索引 Storing values within the index数据存储在 heap file 中,索引处存储 heap file 的 reference,这样可以避免重复存储数据.index 中新值可以 overwrite 老值, index 无需更新; 如果空间不够需要生成新的 heapfile, 则需要更新 index。 clustered index(storing all row data within the index): so it can be desirable to store the indexed row directly within an index. This is known as a clustered index(聚集索引,比如 mysql 主键). covering index:(比如 mysql 主键外其他键上索引) which stores some of a table’s columns within the index Multi-column indexesconcatenated index(比如 mysql 联合唯一键): simply combines several fields into one key by appending one column to another (the index definition specifies in which order the fields are concatenated) Multi-dimensional indexes are a more general way of querying several columns at once, 比如 R 树? Full-text search and fuzzy indexesTo cope with typos in documents or queries, Lucene is able to search text for words within a certain edit distance but in Lucene, the in-memory index is a finite state automaton over the characters in the keys, similar to a trie Keeping everything in memoryBut other in-memory databases aim for durability, which can be achieved with special hardware (such as battery-powered RAM), by writing a log of changes to disk, by writing periodic snapshots to disk, or by replicating the in-memory state to other machines. Counterintuitively, the performance advantage of in-memory databases is not due to the fact that they don’t need to read from disk.Rather, they can be faster because they can avoid the overheads of encoding in-memory data structures in a form that can be written to disk Besides performance, another interesting area for in-memory databases is providing data models that are difficult to implement with disk-based indexes. SummaryLog-structured storage engineHash Index + append only log 分为 segments, 后台 merge 和 compaction, 磁盘利用率低 index 受内存大小限制 难以进行 range query Range Index + SSTable(LSM-Trees 是其进阶) 写入按 k 排序, 写入 mmtable(memory), 写满后刷到 SSTable(disk) range index + scan filter, 不受内存限制, range query 支持良好 后台进行 merge(高效的有序合并) 和 compaction; 磁盘利用率高 相对读慢写快(顺序写入,append-only,不 update,合并时 delete) crash recovery: append-only log 并发问题: 以上 2 个都是单线程写入, segment file 不可变,可以多线程读取; Their key idea is that they systematically turn random-access writes into sequential writes on disk, which enables higher write throughput due to the performance characteristics of hard drives and SSDs. Page-orientated storage engineB-Trees(update-in-place) 按 k 排序,会进行 update(overwrite) 数据只有一份,事务支持强大 读快写慢(随机写入,还有可能 B 树要分裂) crash recovery: write-ahead log concurrency issue: latches (lightweight locks) 二级索引,聚集索引,非聚集索引… Multi-column indexes else 全文索引 模糊索引 Keeping everything in memory OLAP(online analytic processing ) Overview特点: scan 大量的数据, project 一些字段,在这些数据上进行聚合运算 Instead it becomes important to encode data very compactly, to minimize the amount of data that the query needs to read from disk. We discussed how column-oriented storage helps achieve this goal. Data Warehousing通过 ETL 将 TP 系统中的数据以合适的方式收集到 data warehouse 中进行 AP 分析.DW 通常是关系模型,因此使用 sql 分析非常合适。 A data warehouse, by contrast, is a separate database that analysts can query to their hearts’ content, without affecting OLTP operations A big advantage of using a separate data warehouse, rather than querying OLTP systems directly for analytics, is that the data warehouse can be optimized for analytic access patterns. Stars and Snowflakes: Schemas for AnalyticsStars schema(也称维度建模): 中间是事实表,其通常包含很多字段,有些是属性,有些事指向维度表的外键。 Snowflakes schema: 是 Stars 的变体,维度表维度和以被细分为更小的维度 fact table(事实表): Each row of the fact table represents an event that occurred at a particular timedimension table(维度表): the dimensions represent the who, what, where, when, how, and why of the event Column-Oriented Storagerow-oriented storage 对事务支持更好, column-oriented storage 对分析支持更好. The idea behind column-oriented storage is simple: don’t store all the values from one row together, but store all the values from each column together instead. The column-oriented storage layout relies on each column file containing the rows in the same order. Column Compressionwe can further reduce the demands on disk throughput by compressing data bitmap encodingvectorized processing A big bottleneck is the bandwidth for getting data from disk into memory Developers of analytical databases also worry about efficiently using the bandwidth from main memory into the CPU cache Sort Order in Column Storage在列存中顺序无关紧要,存储按照 insert 顺序来就好了.但是仍然可以类似 SSTable 指定顺序用来实现索引: Rather, the data needs to be sorted an entire row at a time, even though it is stored by column. A second column can determine the sort order of any rows that have the same value in the first column. Another advantage of sorted order is that it can help with compression of columns Having multiple sort orders in a column-oriented store is a bit similar to having mul‐ tiple secondary indexes in a row-oriented store. Writing to Column-Oriented Storage面向列,压缩和排序增加了写入成本。可以使用类似 LSM-trees 的方式,将数据写入 memtable 再刷到磁盘,刷到磁盘的时候生成 column-oriented 文件。只是这样以来,查询就需要合并 memtable 和磁盘文件,但是查询优化器会对用户屏蔽这些底层细节。 Aggregation: Data Cubes and Materialized ViewsMaterialized view: is and actual copy of the query results, written to disk,When the underlying data changes, a materialized view needs to be updated A common special case of a materialized view is known as a data cube or OLAP cube, It is a grid of aggregates grouped by different dimensions. The disadvantage is that a data cube doesn’t have the same flexibility as querying the raw data","categories":[{"name":"大数据","slug":"大数据","permalink":"http://example.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"ddia","slug":"ddia","permalink":"http://example.com/tags/ddia/"}]},{"title":"Part 1: CHAPTER 2 Data Models and Query Languages","slug":"Part-1-CHAPTER-2-Data-Models-and-Query-Languages","date":"2019-09-08T05:58:07.000Z","updated":"2022-11-25T06:26:45.205Z","comments":true,"path":"2019/09/08/Part-1-CHAPTER-2-Data-Models-and-Query-Languages/","link":"","permalink":"http://example.com/2019/09/08/Part-1-CHAPTER-2-Data-Models-and-Query-Languages/","excerpt":"","text":"? 有哪些 data model? 它们之间的区别和联系是什么? 各自解决了什么场景下的问题? 如何融合? 每个 data model 对应的 query language 是什么? 有什么特点? 关系型和文档型之间的差异是什么? it’s just a question of whether the schema is explicit (enforced on write) or implicit (handled on read)? 关系型数据库的限制?什么导致了这些限制?如何打破这些限制? Overview许多应用都由分层的 data-model 组成,上层屏蔽下层的复杂性,data-model 决定我们解决问题的思维模式.本节介绍 relational model, document model, graph-based model 3 种 data-model 和其对应的 query language。 Relational model 解决数据之间 many-to-many 关系的问题,但是无法适应所有应用场景,于是 NoSQL datastore 应运而生,主要分为 2 大类: Doucument database: Document databases target use cases where data comes in self-contained documents and relationships between one document and another are rare. Graph databases: Graph databases go in the opposite direction, targeting use cases where anything is potentially related to everything. 适应于一个 data-model 的数据也可以用其他的 data-model 来组织,比如: graph data can be represented in a relational database。用不合适的 data-model 来组织数据会使得数据变得难以使用, That’s why we have different systems for different purposes, not a single one-size-fits-all solution. 定义数据存储格式 -&gt; 在其上定义查询语言. Relational Model Versus Document ModelRelational ModelRelational Model 由 row(tuple) 组成 table(relation),主要的 use case 是 transaction processing 和 batch processing。其屏蔽了底层存储的实现细节,对外提供 sql 这种声明式的 query language. Object-oriented 和 relational model 之间存在差异,需要 orm(mybatis 等), 但是也无法完全屏蔽这种差异. A key insight of the relational model was this: you only need to build a query optimizer once, and then all applications that use the database can benefit from it Document Model适用于数据之间 one-to-many 的关系,对 join 支持比较弱. NoSql(Not Only SQL) 产生驱动因素: A need for greater scalability than relational databases can easily achieve, including very large datasets or very high write throughput 开源优势 Specialized query operations that are not well supported by the relational model Frustration with the restrictiveness of relational schemas, and a desire for a more dynamic and expressive data model Schema flexibility in the document model: schema-on-read (the structure of the data is implicit, and only interpreted when the data is read), schema-on-write(the traditional approach of relational databases, where the schema is explicit and the database ensures all written data conforms to it) schema-on-read approach is advantageous if the items in the collection don’t all have the same structure for some reason Many-to-One and Many-to-Many RelationshipsRemoving such duplication is the key idea behind normalization in databases. Relational model 通过外键来组织 one-to-many 的数据关系,通过 join 来查询数据,这种效率比较低。document model 对 one-to-many 支持良好, In document databases, joins are not needed for one-to-many tree structures, and support for joins is often weak.但是如果数据库不支持 join, 就需要自己在代码层面实现数据的 join 逻辑. The network model: 树形结构,通过 link(类似指针而非外键), 支持多对多和多对一The only way of accessing a record was to follow a path from a root record along these chains of links. This was called an access path. The relational model: 数据被组织为表,没有嵌套的结构,可以在表上增删改查 查询路径由数据库的查询优化器自己选取,无需手动指定 在 many to one 和 many to many 上 2 者没有本质差异,Relationl 使用外键,Document 使用 document reference 主要优势比较: Document model: schema flexibility, better performance due to locality, for some applications it is closer to the data structures used by the application Relational model: better support joins, many-to-one and many-to-many Data locality for queryiesdocument 被连续存储,查询性能比较好; 如果被 split 为多个 table, 查询的开销更大数据本地化的策略被大量使用,比如 column-family Convergence of document and relational databasesA hybrid of the relational and document models is a good route for databases to take in the future. 比如 sql-on-elasticsearch, Phoenix Query languages for dataSqlSql 基于 relatinoal algebra, 是声明式语言,它: sql 将查询逻辑和数据库实现解耦,使得 2 者之间可以独立的变化. it gives the database much more room for automatic optimizations. sql 只是指明了查询模式,对执行顺序没有要求,所以可以方便的并行. because it specifies instructions that must be performed in a particular order. Declarative languages have a better chance of getting faster in parallel execution because they specify only the pattern of the results, not the algorithm that is used to determine the results MapReduce Querying一些 NoSql DB 如 Manogo 使用 MR querying。MapReduce is neither a declarative query language nor a fully imperative query API, but somewhere in between. The map and reduce functions are somewhat restricted in what they are allowed to do. They must be pure functions, 写 mr 程序比写 query 是要难的,A usability problem with MapReduce is that you have to write two carefully coordinated JavaScript functions, which is often harder than writing a single query Graph-Like Data Models适用于 many-to-many 模型,由 vertices 和 edges 组成, vertex 和 edge 不需要固定的 schema. graph 的 vertex 和 edge 不一定需要都是同一类,不同的 vertex 和 edge 之间可能会有各种个样的关系, 而且要在这些 vertex 和 edge 上进行各种关联查询. 如果用 Relational model 来表示,会不直观,比如查询需要写大量的 join,不同的 vertex 需要存储到不同表,且有严格的 schema 限制. 构建灵活,利于演进: Graphs are not limited to such homogeneous data: an equally powerful use of graphs is to provide a consistent way of storing completely different types of objects in a single datastore Graphs are good for evolvability: as you add features to your application, a graph can easily be extended to accommodate changes in your application’s data structures. property graph modelthose features vive graphs a great deal of flexibility for data modeling 没有 schema 限制类型 可以有效的查询一个顶点的出边和入边 可以存储额外的信息来描述边 In the property graph model, each vertex consists of: A unique identifier A set of outgoing edges A set of incoming edges A collection of properties (key-value pairs) Each edge consists of: A unique identifier The vertex at which the edge starts (the tail vertex) The Cypher Query LanguageCypher is a declarative query language for property graphs, created for the Neo4j graph database 如果用关系型 model 来表示图,用 sql 来进行图上的操作, sql 会异常复杂 Triple-store modelThe triple-store model is mostly equivalent to the property graph model, using different words to describe the same ideas In a triple-store, all information is stored in the form of very simple three-part state‐ments: (subject, predicate, object). but fortunately you can use semicolons to say multiple things about the same subject. This makes the Turtle format quite nice and readable: The semantic webRDF is designed for internet-wide data exchange,The Resource Description Framework (RDF) was intended as a mechanism for different web‐sites to publish data in a consistent format, allowing data from different websites to be automatically combined into a web of data—a kind of internet-wide “database of everything.” RDF doesn’t distinguish between properties and edges but just uses predicates for both Cypher’s pattern matching is borrowed from SPARQL. Graph Databases Compared to the Network Model Network 规定了 link 的类型,类似 java: Type link, 而 graph vertex 可以和任意 vertex 建立联系,类似 java: T link,This gives much greater flexibility for applications to adapt to changing requirements. 在 network 中必须沿着 access path 进行数据查找. graph 中 you can refer directly to any vertex by its unique ID, or you can use an index to find vertices with a particular value Network 中 the children of a record were an ordered set, so the database had to maintain that ordering, Graph 中 . In a graph database, vertices and edges are not ordered (you can only sort the results when making a query). Network 中 all queries were imperative, Graph 支持 high-level, declarative query languages such as Cypher or SPARQL The Foundation: Datalog古老的 language, it provides the foundation that later query languages build upon. Cascalog 是其一种实现,用来查询 hadoop 上的数据集. Dtalog 定义 data model 为: predicate(subject, object) Cypher and SPARQL jump in right away with SELECT, but Datalog takes a small step at a time.But it’s a very powerful approach, because rules can be combined and reused in different queries. It’s less convenient for simple one-off queries, but it can cope better if your data is complex. Summary每种 model 适应于不同的场景,趋势是不同的 model 之间也在相互融合。 Relation model: 适应于 many-to-many 和 many-to-one 的模式 良好的 join 支持, schema on write Declarative query language: Sql, 解耦查询逻辑和数据库实现,使得查询优化器得以实现 对于 one-to-many 关系将数据 split 到多个 table 中,通过 join 查询降低数据的获取效率 用 join 来进行 graph 的查询难以理解且费时费力 和 OO 的设计之间存在差异,需要通过 ORM 来弥补 Document model 适应于 one-to-many, join 支持不够 灵活的 schema(schema on read) data locality 带来的查询效率 Declarative query language: MR Graph model 允许任意类型的 vertex 建立任意的 edge,灵活的 schema 非常便于应用扩展演进 Declarative query language: Cypher, SPARQL 支持 graph 上的各种查询","categories":[{"name":"大数据","slug":"大数据","permalink":"http://example.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"ddia","slug":"ddia","permalink":"http://example.com/tags/ddia/"}]},{"title":"Part 1: CHAPTER 1 Reliable, Scalable, and Maintainable Applications","slug":"Part-1-CHAPTER-1-Reliable-Scalable-and-Maintainable-Applications","date":"2019-09-07T05:58:07.000Z","updated":"2022-11-25T07:02:35.753Z","comments":true,"path":"2019/09/07/Part-1-CHAPTER-1-Reliable-Scalable-and-Maintainable-Applications/","link":"","permalink":"http://example.com/2019/09/07/Part-1-CHAPTER-1-Reliable-Scalable-and-Maintainable-Applications/","excerpt":"","text":"? how we think of reliaility, scalability, maintainability? how to achieve reliaility, scalability, maintainability? Thinking about data system 数据系统的特点是什么?一方面,传统的数据系统都为了解决某个特定领域和场景的问题,而现在,这种界限在变得模糊,比如将 kudu 用作消息队列; 另一方面,为满足更多的需求,数据系统在变得复杂,需要组合多种组件来完成系统构建; 需要着重考虑和解决什么问题?数据系统设计需要考虑人,技术等诸多因素,我们可以抽象出 3 个比较重要的关注点: Reliability, Scalability, Maintainability Reliability(可靠&#x2F;用性)Reliability 指的是当有 fault 发生,系统仍可以正常执行的能力,这称为 fault-tolerant 或者 resilient。fault(异常,会导致failure) 不同于 failure(失败,不可用), fault 无法完全避免,所以需要构建容错机制,来 cure fault,prevent failure。Reliability 十分重要,对于 hardware faults, software errors and human errors 有不同的应对方法 hardware faults它的特点如下,一般使用硬件冗余来解决问题,然而大规模的数据应用加剧了这个问题,需要引入软件容错 MTTF &#x3D; 10~50 年,规模大了以后,硬件故障概率很高 随机发生和机器之间独立发生(random and independent) software errors 特点: 并非独立,难以预测 There is no quick solution to the problem of systematic faults in software solve: 加强系统测试 进程隔离 重视监控,measuring,gurantee 等 human errors 特点: 人的行为会导致很多错误,但是对人的行为管控严格的系统又缺乏灵活性,这需要做出权衡 solve: 权限管控&#x2F;灵活性(权衡), 快速恢复错误, 单元测试&#x2F;集成测试, 监控指标, 良好的管理实践与充分的培训 ScalabilityScalability means having strategies for keeping performance good, even when load increases. describing Load and performance(描述问题)为了描述 scalability,我们需要描述系统的 load 和 performance,具体的 load 指标取决一个系统。描述 load 和 performance 之间的关系需要阐明 2 个问题: 保持资源不变的情况下,提高 load, 对系统有何影响? load 提高后,如果要保持 performance, 如何增加资源? 对于 performance 的描述,平均值并不准确,这无法反映用户的实际体验,应该使用分布比率(percentile)来进行衡量,如 P50 approaches for coping with load(解决问题)how do we maintain good performance even when our load parameters increase by some amount? 将有状态服务扩展到多个节点上是困难的,但是趋势如此。load 增长,需要改变系统架构: scale-up(vertical使用大型机) or scale-out(horizontal加机器) 或者 2 者进行结合,架构的扩展性设计基于应用的 load parameters, 但是也有一些通用的原则和设计,之后会进行讨论 Maintainability软件的最大成本在于后学维护,可维护性系统的 3 个设计原则: Operability: 容易运维,这需要系统有良好的监控系统,完善的文档等来保证 Simplicity: 系统易于理解和使用,这需要系统设计具备良好的抽象性 Evolvability: 扩展性良好,基于前 2 点,可以使用 TDD 和 Refactoring 进行指导","categories":[{"name":"大数据","slug":"大数据","permalink":"http://example.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"ddia","slug":"ddia","permalink":"http://example.com/tags/ddia/"}]}],"categories":[{"name":"olap","slug":"olap","permalink":"http://example.com/categories/olap/"},{"name":"云原生","slug":"云原生","permalink":"http://example.com/categories/%E4%BA%91%E5%8E%9F%E7%94%9F/"},{"name":"大数据","slug":"大数据","permalink":"http://example.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"starrocks","slug":"starrocks","permalink":"http://example.com/tags/starrocks/"},{"name":"k8s","slug":"k8s","permalink":"http://example.com/tags/k8s/"},{"name":"容器","slug":"容器","permalink":"http://example.com/tags/%E5%AE%B9%E5%99%A8/"},{"name":"docker","slug":"docker","permalink":"http://example.com/tags/docker/"},{"name":"ddia","slug":"ddia","permalink":"http://example.com/tags/ddia/"}]}